{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Explore Influenciae docs \u00bb Influenciae is a Python toolkit dedicated to computing influence values for the discovery of potentially problematic samples in a dataset and the generation of data-centric explanations for deep learning models. In this library based on Tensorflow, we gather state-of-the-art methods for estimating the importance of training samples and their influence on test data-points for validating the quality of datasets and of the models trained on them. \ud83d\udd25 Tutorials \u00b6 We propose some hands-on tutorials to get familiar with the library and it's API: Getting Started Benchmarking with Mislabeled sample detection Using the first order influence calculator Using the second order influence calculator Using TracIn Using Representer Point Selection - L2 (RPS_L2) Using Representer Point Selection - Local Jacobian Expansion (RPS_LJE) \ud83d\ude80 Quick Start \u00b6 Influenciae requires a version of python 3.7 or higher and several libraries, including Tensorflow and Numpy. Installation can be done using Pypi: pip install influenciae Once Influenciae is installed, there are two major applications for the different modules (that all follow the same API). So, except for group-specific functions that are only available on the influence module, all the classes are able to compute self-influence values, the influence with one point w.r.t. another, as well as find the top-k samples for both of these situations. Discovering influential examples \u00b6 Particularly useful when validating datasets, influence functions (and related notions) allow for gaining an insight into what samples the models thinks to be \"important\". For this, the training dataset and a trained model are needed. from deel.influenciae.common import InfluenceModel , ExactIHVP from deel.influenciae.influence import FirstOrderInfluenceCalculator from deel.influenciae.utils import ORDER # load the model, the training loss (without reduction) and the training data (with the labels and in a batched TF dataset) influence_model = InfluenceModel ( model , target_layer , loss_function ) ihvp_calculator = ExactIHVP ( influence_model , train_dataset ) influence_calculator = FirstOrderInfluenceCalculator ( influence_model , train_dataset , ihvp_calculator ) data_and_influence_dataset = influence_calculator . compute_influence_values ( train_dataset ) # or influence_calculator.compute_top_k_from_training_dataset(train_dataset, k_samples, ORDER.DESCENDING) when the # dataset is too large This is also explained more in depth in the Getting Started tutotial Explaining neural networks through their training data \u00b6 Another application is to explain some model's predictions by looking on which training samples they are based on. Again, the training dataset, the model and the samples we wish to explain are needed. from deel.influenciae.common import InfluenceModel , ExactIHVP from deel.influenciae.influence import FirstOrderInfluenceCalculator from deel.influenciae.utils import ORDER # load the model, the training loss (without reduction), the training data and # the data to explain (with the labels and in batched a TF dataset) influence_model = InfluenceModel ( model , target_layer , loss_function ) ihvp_calculator = ExactIHVP ( influence_model , train_dataset ) influence_calculator = FirstOrderInfluenceCalculator ( influence_model , train_dataset , ihvp_calculator ) data_and_influence_dataset = influence_calculator . estimate_influence_values_in_batches ( samples_to_explain , train_dataset ) # or influence_calculator.top_k(samples_to_explain, train_dataset, k_samples, ORDER.DESCENDING) when the # dataset is too large This is also explained more in depth in the Getting Started tutorial Determining the influence of groups of samples \u00b6 The previous examples use notions of influence that are applied individually to each data-point, but it is possible to extend this to groups. That is, answer the question of what would a model look like if it hadn't seen a whole group of data-points during training, for example. This can be computed namely using the FirstOrderInfluenceCalculator and SecondOrderInfluenceCalculator , for implementations where pairwise interactions between each of the data-points are not taken into account and do, respectively. For obtaining the groups' influence: from deel.influenciae.common import InfluenceModel , ExactIHVP from deel.influenciae.influence import SecondOrderInfluenceCalculator # load the model, the training loss (without reduction), the training data and # the data to explain (with the labels and in a batched TF dataset) influence_model = InfluenceModel ( model , target_layer , loss_function ) ihvp_calculator = ExactIHVP ( influence_model , train_dataset ) influence_calculator = SecondOrderInfluenceCalculator ( influence_model , train_dataset , ihvp_calculator ) # or FirstOrderInfluenceCalculator data_and_influence_dataset = influence_calculator . estimate_influence_values_group ( groups_train , groups_to_explain ) For the data-centric explanations: from deel.influenciae.common import InfluenceModel , ExactIHVP from deel.influenciae.influence import SecondOrderInfluenceCalculator # load the model, the training loss (without reduction), the training data and # the data to explain (with the labels and in a batched TF dataset) influence_model = InfluenceModel ( model , target_layer , loss_function ) ihvp_calculator = ExactIHVP ( influence_model , train_dataset ) influence_calculator = SecondOrderInfluenceCalculator ( influence_model , train_dataset , ihvp_calculator ) # or FirstOrderInfluenceCalculator data_and_influence_dataset = influence_calculator . estimate_influence_values_group ( groups_train ) \ud83d\udce6 What's Included \u00b6 All the influence calculation methods work on Tensorflow models trained for any sort of task and on any type of data. Visualization functionality is implemented for image datasets only (for the moment). Influence Method Source Tutorial Influence Functions Paper RelatIF Paper Influence Functions (first order, groups) Paper Influence Functions (second order, groups) Paper Representer Point Selection (L2) Paper Representer Point Selection (Local Jacobian Expansion) Paper Trac-In Paper \ud83d\udc40 See Also \u00b6 This library proposes implementations of some of the different popular ways of calculating the influence of data-points on TF, but there are also other ones using other frameworks. Some other tools for efficiently computing influence functions. Scaling Up Influence Functions a Python library using JAX implementing a scalable algorithm for computing influence functions. FastIF: Scalable Influence Functions for Efficient Model Interpretation and Debugging a Python library using PyTorch implementing another scalable algorithm for computing influence functions. More from the DEEL project: Xplique a Python library exclusively dedicated to explaining neural networks. deel-lip a Python library for training k-Lipschitz neural networks on TF. deel-torchlip a Python library for training k-Lipschitz neural networks on PyTorch. DEEL White paper a summary of the DEEL team on the challenges of certifiable AI and the role of data quality, representativity and explainability for this purpose. \ud83d\ude4f Acknowledgments \u00b6 This project received funding from the French \u201dInvesting for the Future \u2013 PIA3\u201d program within the Artificial and Natural Intelligence Toulouse Institute (ANITI). The authors gratefully acknowledge the support of the DEEL project. \ud83d\udc68\u200d\ud83c\udf93 Creators \u00b6 This library was first created as a research tool by Agustin Martin PICARD in the context of the DEEL project with the help of David Vigouroux and Thomas FEL . Later on, Lucas Hervier joined the team to transform (at least attempt) the code base as a practical user-(almost)-friendly and efficient tool. \ud83d\udcdd License \u00b6 The package is released under MIT license .","title":"Home"},{"location":"#tutorials","text":"We propose some hands-on tutorials to get familiar with the library and it's API: Getting Started Benchmarking with Mislabeled sample detection Using the first order influence calculator Using the second order influence calculator Using TracIn Using Representer Point Selection - L2 (RPS_L2) Using Representer Point Selection - Local Jacobian Expansion (RPS_LJE)","title":"\ud83d\udd25 Tutorials"},{"location":"#quick-start","text":"Influenciae requires a version of python 3.7 or higher and several libraries, including Tensorflow and Numpy. Installation can be done using Pypi: pip install influenciae Once Influenciae is installed, there are two major applications for the different modules (that all follow the same API). So, except for group-specific functions that are only available on the influence module, all the classes are able to compute self-influence values, the influence with one point w.r.t. another, as well as find the top-k samples for both of these situations.","title":"\ud83d\ude80 Quick Start"},{"location":"#whats-included","text":"All the influence calculation methods work on Tensorflow models trained for any sort of task and on any type of data. Visualization functionality is implemented for image datasets only (for the moment). Influence Method Source Tutorial Influence Functions Paper RelatIF Paper Influence Functions (first order, groups) Paper Influence Functions (second order, groups) Paper Representer Point Selection (L2) Paper Representer Point Selection (Local Jacobian Expansion) Paper Trac-In Paper","title":"\ud83d\udce6 What's Included"},{"location":"#see-also","text":"This library proposes implementations of some of the different popular ways of calculating the influence of data-points on TF, but there are also other ones using other frameworks. Some other tools for efficiently computing influence functions. Scaling Up Influence Functions a Python library using JAX implementing a scalable algorithm for computing influence functions. FastIF: Scalable Influence Functions for Efficient Model Interpretation and Debugging a Python library using PyTorch implementing another scalable algorithm for computing influence functions. More from the DEEL project: Xplique a Python library exclusively dedicated to explaining neural networks. deel-lip a Python library for training k-Lipschitz neural networks on TF. deel-torchlip a Python library for training k-Lipschitz neural networks on PyTorch. DEEL White paper a summary of the DEEL team on the challenges of certifiable AI and the role of data quality, representativity and explainability for this purpose.","title":"\ud83d\udc40 See Also"},{"location":"#acknowledgments","text":"This project received funding from the French \u201dInvesting for the Future \u2013 PIA3\u201d program within the Artificial and Natural Intelligence Toulouse Institute (ANITI). The authors gratefully acknowledge the support of the DEEL project.","title":"\ud83d\ude4f Acknowledgments"},{"location":"#creators","text":"This library was first created as a research tool by Agustin Martin PICARD in the context of the DEEL project with the help of David Vigouroux and Thomas FEL . Later on, Lucas Hervier joined the team to transform (at least attempt) the code base as a practical user-(almost)-friendly and efficient tool.","title":"\ud83d\udc68\u200d\ud83c\udf93 Creators"},{"location":"#license","text":"The package is released under MIT license .","title":"\ud83d\udcdd License"},{"location":"api/benchmarks/","text":"Benchmarks \u00b6 This module allows users to easily and objectively compare the performance of the different techniques that have been implemented in this library, or new ones that may come up. The benchmark is based on a widespread test used in the literature to measure the different methods' capacity to find problematic data-points: train a model on a dataset that has a certain percentage of mislabeled data-points (that are already known for validation purposes), and use the influence values to rank them and see what percentage of the whole dataset must be checked to find all the mislabeled points when ordered that way. In particular, a good metric for this is the AUC (area under the curve) of the ROC curve of this operation. Additionally, as we are working with stochastic algorithms, we need to make sure that the scores we obtain have a statistical value. This means that this operation should be repeated over multiple seeds to make sure that the method really does perform better than others. For this reason, a training procedure must be created so as to be able to automatically repeat the process of training the model on noisy versions of the original dataset. In particular, this module includes interfaces for training procedures and mislabeled sample detection benchmarking, and an implementation on the popular CIFAR10 image-classification dataset. Notebooks \u00b6 Benchmarking with Mislabeled sample detection Cifar10TrainingProcedure \u00b6 A class for streamlining the process of training models on the CIFAR-10 dataset. __init__( self , epochs : int = 60 , model_type : str = 'resnet' , use_regu : bool = True , force_overfit : bool = False , epochs_to_save : Optional[List[int]] = None , verbose : bool = True , use_tensorboard : bool = False) \u00b6 Parameters epochs : int = 60 An integer indicating for how long the model should be trained model_type : str = 'resnet' A string with the type of model to use. Either 'resnet', 'efficient_net' or 'vgg19'. use_regu : bool = True A boolean indicating whether L1L2 regularization should be used on the last layer. force_overfit : bool = False A boolean for if the training schedule to be used should try to overfit the model or not. epochs_to_save : Optional[List[int]] = None A list of integers for the eventual saving of the model's checkpoints and training information. Useful for computing influence-related quantities using TracIn. verbose : bool = True A boolean indicating whether to print the progress to stdout. use_tensorboard : bool = False A boolean indicating the use of tensorboard for logging. train( self , training_dataset : tf.Dataset , test_dataset : tf.Dataset , train_batch_size : int = 128 , test_batch_size : int = 128 , log_path : Optional[str] = None) -> Tuple[float, float, keras.engine.training.Model, Any] \u00b6 Trains the desired model with the object's training schedule on the provided training dataset, and validates its performance on the test dataset. Optionally, it logs the results progressively using the tensorboard framework. Parameters training_dataset : tf.Dataset A TF dataset containing the samples on which the model will be trained. Typically, for this benchmark, it will be a noisy version of CIFAR-10. test_dataset : tf.Dataset A TF dataset with the test data on which to validate the model's accuracy. This data should only contain clean samples (i.e. no flipped labels). train_batch_size : int = 128 An integer specifying the batch size for the training procedure. test_batch_size : int = 128 An integer specifying the batch size for the validation procedure. log_path : Optional[str] = None An (optional) string with the path onto which to save the tensorboard logs. Return train_stats, test_stats, model, model_saver_information : Tuple[float, float, keras.engine.training.Model, Any] A tuple with the model's train accuracy, test accuracy, final model and the model saver information set to None for now Cifar10MislabelingDetectorEvaluator \u00b6 An implementation of the mislabeled sample detection benchmark on the CIFAR10 image classification dataset. Useful for streamlining the evaluation of the different influence calculator techniques. __init__( self , epochs : int = 60 , model_type : str = 'resnet' , mislabeling_ratio : float = 0.0005 , use_regu : bool = True , force_overfit : bool = False , train_batch_size : int = 128 , test_batch_size : int = 128 , influence_batch_size : Optional[int] = None , epochs_to_save : Optional[List[int]] = None , take_batch : Optional[int] = None , verbose_training : bool = True , use_tensorboard : bool = False) \u00b6 Parameters epochs : int = 60 An integer indicating for how long the model should be trained model_type : str = 'resnet' A string with the type of model to use. Either 'resnet', 'efficient_net' or 'vgg19'. mislabeling_ratio : float = 0.0005 A float with the ratio of noise to add to the training dataset's labels (ranging from 0. to 1.). use_regu : bool = True A boolean indicating whether L1L2 regularization should be used on the last layer. force_overfit : bool = False A boolean for if the training schedule to be used should try to overfit the model or not. train_batch_size : int = 128 An integer with the size of the batches on which to train the model. test_batch_size : int = 128 An integer with the size of the batches on which to perform the validation. epochs_to_save : Optional[List[int]] = None A list of integers for the eventual saving of the model's checkpoints and training information. Useful for computing influence-related quantities using TracIn. take_batch : Optional[int] = None An optional integer indicating the number of batches to take if only one part of the training and test datasets are to be used. verbose_training : bool = True A boolean indicating if progress about the training procedures should be reported to stdout. use_tensorboard : bool = False A boolean indicating the use of tensorboard for logging. bench( self , influence_calculator_factories : Dict[str, deel.influenciae.benchmark.influence_factory.InfluenceCalculatorFactory] , nbr_of_evaluation : int , path_to_save : str , seed : int = 0 , verbose : bool = True , use_tensorboard : bool = False) -> Dict[str, Tuple[ , , float]] \u00b6 Performs the whole benchmark for a group of influence calculator techniques and a number of evaluations for each of them (for statistical significance). Parameters influence_calculator_factories : Dict[str, deel.influenciae.benchmark.influence_factory.InfluenceCalculatorFactory] A dictionary with the name of the influence calculator technique and a factory for creating instances for them. nbr_of_evaluation : int An integer with the amount of evaluations per method. path_to_save : str A string specifying the path to save the results. seed : int = 0 An integer for setting the seed on all the random number generators. verbose : bool = True A boolean indicating whether progress is reported in stdout or not. use_tensorboard : bool = False A boolean indicating if the results are to be progressively logged into tensorboard. Return result : Dict[str, Tuple[ , , float]] A dictionary with the name of each method and its results. build_noisy_training_dataset( self ) -> Tuple[tf.Dataset, ] \u00b6 Generates a noisy version of the object's own dataset. In particular, it will include noise in the label information (i.e. the label will be switched at random). More noise will effectively impact the model's capacity to predict correctly on the test set, as it will need to learn to use spurious correlations to attain the 100% accuracy on the training dataset. Return noisy_dataset, noise_indexes : Tuple[tf.Dataset, ] A tuple with the noisy dataset and a numpy array with the flipped labels (used for validation during the evaluation). evaluate( self , influence_factory : deel.influenciae.benchmark.influence_factory.InfluenceCalculatorFactory , nbr_of_evaluation : int , seed : int = 0 , verbose : bool = True , path_to_save : Optional[str] = None , use_tensorboard : bool = False , method_name : Optional[str] = None) -> Tuple[ , , float] \u00b6 Performs one benchmark evaluation over one influence calculator technique the specified number of times (for statistical significance). Parameters influence_factory : deel.influenciae.benchmark.influence_factory.InfluenceCalculatorFactory A factory for instantiating objects for a given influence calculator technique. nbr_of_evaluation : int An integer with the amount of evaluations per method. seed : int = 0 An integer for setting the seed on all the random number generators. verbose : bool = True A boolean indicating whether progress is reported in stdout or not. path_to_save : Optional[str] = None A string specifying the path to save the results. use_tensorboard : bool = False A boolean indicating if the results are to be progressively logged into tensorboard. method_name : Optional[str] = None An optional string with the experience's name. Return curves, mean_curve, roc : Tuple[ , , float] A tuple with the experience's results: (each of the individual curves, the mean curve, the ROC) plot_tensorboard_roc( curve : numpy.ndarray , experiment_name : str) \u00b6 Plots a mislabeled samples detection ROC curve on tensorboard. Parameters curve : numpy.ndarray A numpy array with the experiment's curve experiment_name : str A string with the experiment's name set_seed( seed : int) \u00b6 Sets all the random seeds on TensorFlow, numpy and python for traceability. Parameters seed : int An integer with the seed value.","title":"Benchmarks"},{"location":"api/benchmarks/#benchmarks","text":"This module allows users to easily and objectively compare the performance of the different techniques that have been implemented in this library, or new ones that may come up. The benchmark is based on a widespread test used in the literature to measure the different methods' capacity to find problematic data-points: train a model on a dataset that has a certain percentage of mislabeled data-points (that are already known for validation purposes), and use the influence values to rank them and see what percentage of the whole dataset must be checked to find all the mislabeled points when ordered that way. In particular, a good metric for this is the AUC (area under the curve) of the ROC curve of this operation. Additionally, as we are working with stochastic algorithms, we need to make sure that the scores we obtain have a statistical value. This means that this operation should be repeated over multiple seeds to make sure that the method really does perform better than others. For this reason, a training procedure must be created so as to be able to automatically repeat the process of training the model on noisy versions of the original dataset. In particular, this module includes interfaces for training procedures and mislabeled sample detection benchmarking, and an implementation on the popular CIFAR10 image-classification dataset.","title":"Benchmarks"},{"location":"api/benchmarks/#notebooks","text":"Benchmarking with Mislabeled sample detection","title":"Notebooks"},{"location":"api/benchmarks/#Cifar10TrainingProcedure","text":"A class for streamlining the process of training models on the CIFAR-10 dataset.","title":"Cifar10TrainingProcedure"},{"location":"api/benchmarks/#Cifar10MislabelingDetectorEvaluator","text":"An implementation of the mislabeled sample detection benchmark on the CIFAR10 image classification dataset. Useful for streamlining the evaluation of the different influence calculator techniques.","title":"Cifar10MislabelingDetectorEvaluator"},{"location":"api/tracin/","text":"TracIn \u00b6 View source | \ud83d\udcf0 Original Paper This method proposes an alternative for estimating influence without the need for expensive inverse hessian-vector product computations, but requiring information that is only available at train time. It leverages the fundamental theorem of calculus to estimate the influence of the training points by looking at how the loss at that point evolves at different model checkpoints. Concretely, the influence will take the following for: \\[ \\mathcal{I} (z, z') = \\sum_i^k \\eta_i \\nabla_\\theta \\ell (\\theta_{t_i}, z) \\cdot \\nabla_\\theta \\ell (\\theta_{t_i}, z') \\] where \\(\\theta_{t_i}\\) are the model's weights at epoch \\(t_i\\) and \\(\\eta_i\\) is the learning rate at that same epoch. Just like RPS-L2, this method does not need an instance of the InverseHessianVectorProduct class, but does require to provide some of the model's checkpoints and the learning rates at each of them. Notebooks \u00b6 Using TracIn TracIn \u00b6 A class implementing an influence score based on TracIn method proposed in https://arxiv.org/pdf/2002.08484.pdf __init__( self , models : List[deel.influenciae.common.model_wrappers.InfluenceModel] , learning_rates : Union[float, List[float]]) \u00b6 Parameters models : List[deel.influenciae.common.model_wrappers.InfluenceModel] A list of TF2.X models implementing the InfluenceModel interface at different steps (epochs) of the training learning_rates : Union[float, List[float]] Learning rate or list of learning rates used during the training. If learning_rates is a list, it should have the same size as the amount of models compute_influence_values( self , train_set : tf.Dataset , device : Optional[str] = None) -> tf.Dataset \u00b6 Compute the influence score for each sample of the provided (full or partial) model's training dataset. Parameters train_set : tf.Dataset A TF dataset with the (full or partial) model's training dataset. device : Optional[str] = None Device where the computation will be executed Return train_set : tf.Dataset A dataset containing the tuple: (batch of training samples, influence score) compute_influence_vector( self , train_set : tf.Dataset , save_influence_vector_ds_path : Optional[str] = None , device : Optional[str] = None) -> tf.Dataset \u00b6 Compute the influence vector for each sample of the provided (full or partial) model's training dataset. Parameters train_set : tf.Dataset A TF dataset with the (full or partial) model's training dataset. save_influence_vector_ds_path : Optional[str] = None The path to save or load the influence vector of the training dataset. If specified, load the dataset if it has already been computed, otherwise, compute the influence vector and then save it in the specified path. device : Optional[str] = None Device where the computation will be executed Return inf_vect_ds : tf.Dataset A dataset containing the tuple: (batch of training samples, influence vector) compute_top_k_from_training_dataset( self , train_set : tf.Dataset , k : int , order: deel.influenciae.utils.sorted_dict.ORDER = ) -> Tuple[tf.Tensor, tf.Tensor] \u00b6 Compute the k most influential data-points of the model's training dataset by computing Cook's distance for each point individually. Parameters train_set : tf.Dataset A TF dataset containing the points on which the model was trained. k : int An integer with the number of most important samples we wish to keep order : 2> Either ORDER.DESCENDING or ORDER.ASCENDING depending on if we wish to find the top-k or bottom-k samples, respectively. Return training_samples, influences_values : Tuple[tf.Tensor, tf.Tensor] A tuple of tensor. - training_samples: A tensor containing the k most influential samples of the training dataset for the model provided. - influences_values: The influence score corresponding to these k most influential samples. estimate_influence_values_in_batches( self , dataset_to_evaluate : tf.Dataset , train_set : tf.Dataset , influence_vector_in_cache: deel.influenciae.common.base_influence.CACHE = , load_influence_vector_path : Optional[str] = None , save_influence_vector_path : Optional[str] = None , save_influence_value_path : Optional[str] = None , device : Optional[str] = None) -> tf.Dataset \u00b6 Estimates the influence that each point in the provided training dataset has on each of the test points. This can provide some insights as to what makes the model predict a certain way for the given test points, and thus presents data-centric explanations. Parameters dataset_to_evaluate : tf.Dataset A TF dataset containing the test samples for which to compute the effect of removing each of the provided training points (individually). train_set : tf.Dataset A TF dataset containing the model's training dataset (partial or full). influence_vector_in_cache : 0> An enum indicating if intermediary values are to be cached (either in memory or on the disk) or not. Options include CACHE.MEMORY (0) for caching in memory, CACHE.DISK (1) for the disk and CACHE.NO_CACHE (2) for no optimization. load_influence_vector_path : Optional[str] = None The path to load the influence vectors (if they have already been calculated). save_influence_vector_path : Optional[str] = None The path to save the computed influence vector. save_influence_value_path : Optional[str] = None The path to save the computed influence values. device : Optional[str] = None Device where the computation will be executed Return influence_value_dataset : tf.Dataset A dataset containing the tuple: (samples_to_evaluate, dataset). - samples_to_evaluate: The batch of sample to evaluate. - dataset: Dataset containing tuples of batch of the training dataset and their influence score. top_k( self , dataset_to_evaluate : tf.Dataset , train_set : tf.Dataset , k : int = 5 , nearest_neighbors : deel.influenciae.utils.nearest_neighbors.BaseNearestNeighbors = , influence_vector_in_cache: deel.influenciae.common.base_influence.CACHE = , load_influence_vector_ds_path : Optional[str] = None , save_influence_vector_ds_path : Optional[str] = None , save_top_k_ds_path : Optional[str] = None , order: deel.influenciae.utils.sorted_dict.ORDER = , d_type : tensorflow.python.framework.dtypes.DType = tf.float32 , device : Optional[str] = None) -> tf.Dataset \u00b6 Find the top-k closest elements for each element of dataset to evaluate in the training dataset The method will return a dataset containing a tuple of: (Top-k influence values for each sample to evaluate, Top-k training sample for each sample to evaluate) Parameters dataset_to_evaluate : tf.Dataset The dataset which contains the samples which will be compare to the training dataset train_set : tf.Dataset The dataset used to train the model. k : int = 5 the number of most influence samples to retain in training dataset nearest_neighbors : deel.influenciae.utils.nearest_neighbors.BaseNearestNeighbors = The nearest neighbor method. The default method is a linear search influence_vector_in_cache : 0> An enum indicating if intermediary values are to be cached (either in memory or on the disk) or not. Options include CACHE.MEMORY (0) for caching in memory, CACHE.DISK (1) for the disk and CACHE.NO_CACHE (2) for no optimization. load_influence_vector_ds_path : Optional[str] = None The path to load the influence vectors (if they have already been calculated). save_influence_vector_ds_path : Optional[str] = None The path to save the computed influence vector. save_top_k_ds_path : Optional[str] = None The path to save the result of the computation of the top-k elements order : 2> Either ORDER.DESCENDING or ORDER.ASCENDING depending on if we wish to find the top-k or bottom-k samples, respectively. d_type : tensorflow.python.framework.dtypes.DType = tf.float32 The data-type of the tensors. device : Optional[str] = None Device where the computation will be executed Return top_k_dataset : tf.Dataset A dataset containing the tuple (samples_to_evaluate, influence_values, training_samples). - samples_to_evaluate: Top-k samples to evaluate. - influence_values: Top-k influence values for each sample to evaluate. - training_samples: Top-k training sample for each sample to evaluate.","title":"TracIn"},{"location":"api/tracin/#tracin","text":"View source | \ud83d\udcf0 Original Paper This method proposes an alternative for estimating influence without the need for expensive inverse hessian-vector product computations, but requiring information that is only available at train time. It leverages the fundamental theorem of calculus to estimate the influence of the training points by looking at how the loss at that point evolves at different model checkpoints. Concretely, the influence will take the following for: \\[ \\mathcal{I} (z, z') = \\sum_i^k \\eta_i \\nabla_\\theta \\ell (\\theta_{t_i}, z) \\cdot \\nabla_\\theta \\ell (\\theta_{t_i}, z') \\] where \\(\\theta_{t_i}\\) are the model's weights at epoch \\(t_i\\) and \\(\\eta_i\\) is the learning rate at that same epoch. Just like RPS-L2, this method does not need an instance of the InverseHessianVectorProduct class, but does require to provide some of the model's checkpoints and the learning rates at each of them.","title":"TracIn"},{"location":"api/tracin/#notebooks","text":"Using TracIn","title":"Notebooks"},{"location":"api/tracin/#TracIn","text":"A class implementing an influence score based on TracIn method proposed in https://arxiv.org/pdf/2002.08484.pdf","title":"TracIn"},{"location":"api/influence/first_order_influence_calculator/","text":"First Order Influence Calculator \u00b6 View source | \ud83d\udcf0 Original Paper | \ud83d\udcf0 Paper Groups | \ud83d\udcf0 Paper RelatIF | This method is an implementation of the famous technique introduced by Koh & Liang in 2017. In essence, by performing a first-order taylor approximation, it proposes that the influence function of a neural network model can be computed as follows: \\[ \\mathcal{I} (z) \\approx H_{\\hat{\\theta}}^{-1} \\, \\nabla_\\theta \\ell (\\hat{\\theta}, z), \\] where \\(H_{\\hat{\\theta}}^{-1}\\) is the inverse of the mean hessian of the loss wrt the model's parameters over the whole dataset, \\(\\ell\\) is the loss function with which the model was trained and \\(z\\) , a point we wish to leave out of the training dataset. In particular, this computation is carried out by the InverseHessianVectorProduct class, which allows to do it in different ways, with each implementation having its pros and cons. It can be used to compute the self-influence of individual and groups of points, and the influence of training points (or groups) on other test points (or groups). It also implements the RelatIF technique, which can be computed by setting the normalize attribute to True . Notebooks \u00b6 Getting started Using the first order influence calculator FirstOrderInfluenceCalculator \u00b6 A class implementing the necessary methods to compute the different influence quantities using a first-order approximation. This makes it ideal for individual points and small groups of data, as it does so (relatively) efficiently. __init__( self , model : deel.influenciae.common.model_wrappers.InfluenceModel , dataset : tf.Dataset , ihvp_calculator : Union[str, deel.influenciae.common.inverse_hessian_vector_product.InverseHessianVectorProduct, deel.influenciae.common.inverse_hessian_vector_product.IHVPCalculator] = 'exact' , n_samples_for_hessian : Optional[int] = None , shuffle_buffer_size : Optional[int] = 10000 , normalize=False) \u00b6 Parameters model : deel.influenciae.common.model_wrappers.InfluenceModel The TF2.X model implementing the InfluenceModel interface. dataset : tf.Dataset A batched TF dataset containing the training dataset over which we will estimate the inverse-hessian-vector product. ihvp_calculator : Union[str, deel.influenciae.common.inverse_hessian_vector_product.InverseHessianVectorProduct, deel.influenciae.common.inverse_hessian_vector_product.IHVPCalculator] = 'exact' Either a string containing the IHVP method ('exact' or 'cgd'), an IHVPCalculator object or an InverseHessianVectorProduct object. n_samples_for_hessian : Optional[int] = None An integer indicating the amount of samples to take from the provided train dataset. shuffle_buffer_size : Optional[int] = 10000 An integer indicating the buffer size of the train dataset's shuffle operation -- when choosing the amount of samples for the hessian. normalize : normalize=False Implement \"RelatIF: Identifying Explanatory Training Examples via Relative Influence\" https://arxiv.org/pdf/2003.11630.pdf if True, compute the relative influence by normalizing the influence function. assert_compatible_datasets( dataset_a : tf.Dataset , dataset_b : tf.Dataset) -> int \u00b6 Assert that the datasets are compatible: that they contain the same number of points. Else, throw an error. Parameters dataset_a : tf.Dataset First batched tensorflow dataset to check. dataset_b : tf.Dataset Second batched tensorflow dataset to check. Return size : int The size of the dataset. compute_influence_values( self , train_set : tf.Dataset , device : Optional[str] = None) -> tf.Dataset \u00b6 Compute the influence score for each sample of the provided (full or partial) model's training dataset. Parameters train_set : tf.Dataset A TF dataset with the (full or partial) model's training dataset. device : Optional[str] = None Device where the computation will be executed Return train_set : tf.Dataset A dataset containing the tuple: (batch of training samples, influence score) compute_influence_vector( self , train_set : tf.Dataset , save_influence_vector_ds_path : Optional[str] = None , device : Optional[str] = None) -> tf.Dataset \u00b6 Compute the influence vector for each sample of the provided (full or partial) model's training dataset. Parameters train_set : tf.Dataset A TF dataset with the (full or partial) model's training dataset. save_influence_vector_ds_path : Optional[str] = None The path to save or load the influence vector of the training dataset. If specified, load the dataset if it has already been computed, otherwise, compute the influence vector and then save it in the specified path. device : Optional[str] = None Device where the computation will be executed Return inf_vect_ds : tf.Dataset A dataset containing the tuple: (batch of training samples, influence vector) compute_influence_vector_group( self , group : tf.Dataset) -> tf.Tensor \u00b6 Computes the influence function vector -- an estimation of the weights difference when removing the points -- of the whole group of points. Parameters group : tf.Dataset A batched TF dataset containing the group of points of which we wish to compute the influence of removal. Return influence_group : tf.Tensor A tensor containing one vector for the whole group. compute_top_k_from_training_dataset( self , train_set : tf.Dataset , k : int , order: deel.influenciae.utils.sorted_dict.ORDER = ) -> Tuple[tf.Tensor, tf.Tensor] \u00b6 Compute the k most influential data-points of the model's training dataset by computing Cook's distance for each point individually. Parameters train_set : tf.Dataset A TF dataset containing the points on which the model was trained. k : int An integer with the number of most important samples we wish to keep order : 2> Either ORDER.DESCENDING or ORDER.ASCENDING depending on if we wish to find the top-k or bottom-k samples, respectively. Return training_samples, influences_values : Tuple[tf.Tensor, tf.Tensor] A tuple of tensor. - training_samples: A tensor containing the k most influential samples of the training dataset for the model provided. - influences_values: The influence score corresponding to these k most influential samples. estimate_influence_values_group( self , group_train : tf.Dataset , group_to_evaluate : Optional[tf.Dataset] = None) -> tf.Tensor \u00b6 Computes Cook's distance of the whole group of points provided, giving measure of the influence that the group carries on the model's weights. Parameters group_train : tf.Dataset A batched TF dataset containing the group of points we wish to remove. group_to_evaluate : Optional[tf.Dataset] = None A batched TF dataset containing the group of points with respect to whom we wish to measure the influence of removing the training points. Return influence_values_group : tf.Tensor A tensor containing one influence value for the whole group. estimate_influence_values_in_batches( self , dataset_to_evaluate : tf.Dataset , train_set : tf.Dataset , influence_vector_in_cache: deel.influenciae.common.base_influence.CACHE = , load_influence_vector_path : Optional[str] = None , save_influence_vector_path : Optional[str] = None , save_influence_value_path : Optional[str] = None , device : Optional[str] = None) -> tf.Dataset \u00b6 Estimates the influence that each point in the provided training dataset has on each of the test points. This can provide some insights as to what makes the model predict a certain way for the given test points, and thus presents data-centric explanations. Parameters dataset_to_evaluate : tf.Dataset A TF dataset containing the test samples for which to compute the effect of removing each of the provided training points (individually). train_set : tf.Dataset A TF dataset containing the model's training dataset (partial or full). influence_vector_in_cache : 0> An enum indicating if intermediary values are to be cached (either in memory or on the disk) or not. Options include CACHE.MEMORY (0) for caching in memory, CACHE.DISK (1) for the disk and CACHE.NO_CACHE (2) for no optimization. load_influence_vector_path : Optional[str] = None The path to load the influence vectors (if they have already been calculated). save_influence_vector_path : Optional[str] = None The path to save the computed influence vector. save_influence_value_path : Optional[str] = None The path to save the computed influence values. device : Optional[str] = None Device where the computation will be executed Return influence_value_dataset : tf.Dataset A dataset containing the tuple: (samples_to_evaluate, dataset). - samples_to_evaluate: The batch of sample to evaluate. - dataset: Dataset containing tuples of batch of the training dataset and their influence score. top_k( self , dataset_to_evaluate : tf.Dataset , train_set : tf.Dataset , k : int = 5 , nearest_neighbors : deel.influenciae.utils.nearest_neighbors.BaseNearestNeighbors = , influence_vector_in_cache: deel.influenciae.common.base_influence.CACHE = , load_influence_vector_ds_path : Optional[str] = None , save_influence_vector_ds_path : Optional[str] = None , save_top_k_ds_path : Optional[str] = None , order: deel.influenciae.utils.sorted_dict.ORDER = , d_type : tensorflow.python.framework.dtypes.DType = tf.float32 , device : Optional[str] = None) -> tf.Dataset \u00b6 Find the top-k closest elements for each element of dataset to evaluate in the training dataset The method will return a dataset containing a tuple of: (Top-k influence values for each sample to evaluate, Top-k training sample for each sample to evaluate) Parameters dataset_to_evaluate : tf.Dataset The dataset which contains the samples which will be compare to the training dataset train_set : tf.Dataset The dataset used to train the model. k : int = 5 the number of most influence samples to retain in training dataset nearest_neighbors : deel.influenciae.utils.nearest_neighbors.BaseNearestNeighbors = The nearest neighbor method. The default method is a linear search influence_vector_in_cache : 0> An enum indicating if intermediary values are to be cached (either in memory or on the disk) or not. Options include CACHE.MEMORY (0) for caching in memory, CACHE.DISK (1) for the disk and CACHE.NO_CACHE (2) for no optimization. load_influence_vector_ds_path : Optional[str] = None The path to load the influence vectors (if they have already been calculated). save_influence_vector_ds_path : Optional[str] = None The path to save the computed influence vector. save_top_k_ds_path : Optional[str] = None The path to save the result of the computation of the top-k elements order : 2> Either ORDER.DESCENDING or ORDER.ASCENDING depending on if we wish to find the top-k or bottom-k samples, respectively. d_type : tensorflow.python.framework.dtypes.DType = tf.float32 The data-type of the tensors. device : Optional[str] = None Device where the computation will be executed Return top_k_dataset : tf.Dataset A dataset containing the tuple (samples_to_evaluate, influence_values, training_samples). - samples_to_evaluate: Top-k samples to evaluate. - influence_values: Top-k influence values for each sample to evaluate. - training_samples: Top-k training sample for each sample to evaluate.","title":"First Order Influence Calculator"},{"location":"api/influence/first_order_influence_calculator/#first-order-influence-calculator","text":"View source | \ud83d\udcf0 Original Paper | \ud83d\udcf0 Paper Groups | \ud83d\udcf0 Paper RelatIF | This method is an implementation of the famous technique introduced by Koh & Liang in 2017. In essence, by performing a first-order taylor approximation, it proposes that the influence function of a neural network model can be computed as follows: \\[ \\mathcal{I} (z) \\approx H_{\\hat{\\theta}}^{-1} \\, \\nabla_\\theta \\ell (\\hat{\\theta}, z), \\] where \\(H_{\\hat{\\theta}}^{-1}\\) is the inverse of the mean hessian of the loss wrt the model's parameters over the whole dataset, \\(\\ell\\) is the loss function with which the model was trained and \\(z\\) , a point we wish to leave out of the training dataset. In particular, this computation is carried out by the InverseHessianVectorProduct class, which allows to do it in different ways, with each implementation having its pros and cons. It can be used to compute the self-influence of individual and groups of points, and the influence of training points (or groups) on other test points (or groups). It also implements the RelatIF technique, which can be computed by setting the normalize attribute to True .","title":"First Order Influence Calculator"},{"location":"api/influence/first_order_influence_calculator/#notebooks","text":"Getting started Using the first order influence calculator","title":"Notebooks"},{"location":"api/influence/first_order_influence_calculator/#FirstOrderInfluenceCalculator","text":"A class implementing the necessary methods to compute the different influence quantities using a first-order approximation. This makes it ideal for individual points and small groups of data, as it does so (relatively) efficiently.","title":"FirstOrderInfluenceCalculator"},{"location":"api/influence/second_order_influence_calculator/","text":"Second Order Influence Calculator \u00b6 View source | \ud83d\udcf0 Original Paper When working with groups of data, it can prove useful to take into account the pairwise interactions in terms of influence when leaving out large groups of data-points. Basu et al. have thus introduced a second-order formulation that takes these interactions into account: \\[ \\mathcal{I}^{(2)} (\\mathcal{U}, z_t) = \\nabla_\\theta \\ell (\\hat{\\theta}, z_t) \\left(\\mathcal{I}^{(1)} (\\mathcal{U}) + \\mathcal{I}' (\\mathcal{U})\\right) \\] \\[ \\mathcal{I}^{(1)} (\\mathcal{U}) = \\frac{1 - 2 p}{(1 - p)^2} \\frac{1}{|\\mathcal{S}|} H_{\\hat{\\theta}}^{-1} \\sum_{z \\in \\mathcal{U}} \\nabla_\\theta \\ell (\\hat{\\theta}, z) \\] \\[ \\mathcal{I}' (\\mathcal{U}) = \\frac{1}{(1 - p)^2} \\frac{1}{|S|^2} \\sum_{z \\in \\mathcal{U}} H_{\\hat{\\theta}}^{-1} \\nabla_\\theta^2 (\\hat{\\theta}, z) H_{\\hat{\\theta}}^{-1} \\sum_{z' \\in \\mathcal{U}} \\nabla_\\theta \\ell (\\hat{\\theta}, z') \\] As with the rest of the methods based on calculating inverse-hessian-vector products, an important part of the computations are carried out by objects from the class InverseHessianVectorProduct . Notebooks \u00b6 Using the second order influence calculator SecondOrderInfluenceCalculator \u00b6 A class implementing the necessary methods to compute the different influence quantities (only for groups) using a second-order approximation, thus allowing us to take into account the pairwise interactions between points inside the group. For small groups of points, consider using the first order alternative if the computational cost is too high. __init__( self , model : deel.influenciae.common.model_wrappers.InfluenceModel , dataset : tf.Dataset , ihvp_calculator : Union[str, deel.influenciae.common.inverse_hessian_vector_product.InverseHessianVectorProduct, deel.influenciae.common.inverse_hessian_vector_product.IHVPCalculator] = 'exact' , n_samples_for_hessian : Optional[int] = None , shuffle_buffer_size : Optional[int] = 10000) \u00b6 Parameters model : deel.influenciae.common.model_wrappers.InfluenceModel The TF2.X model implementing the InfluenceModel interface. dataset : tf.Dataset A batched TF dataset containing the training dataset over which we will estimate the inverse-hessian-vector product. ihvp_calculator : Union[str, deel.influenciae.common.inverse_hessian_vector_product.InverseHessianVectorProduct, deel.influenciae.common.inverse_hessian_vector_product.IHVPCalculator] = 'exact' Either a string containing the IHVP method ('exact' or 'cgd'), an IHVPCalculator object or an InverseHessianVectorProduct object. n_samples_for_hessian : Optional[int] = None An integer indicating the amount of samples to take from the provided train dataset. shuffle_buffer_size : Optional[int] = 10000 An integer indicating the buffer size of the train dataset's shuffle operation -- when choosing the amount of samples for the hessian. assert_compatible_datasets( dataset_a : tf.Dataset , dataset_b : tf.Dataset) -> int \u00b6 Assert that the datasets are compatible: that they contain the same number of points. Else, throw an error. Parameters dataset_a : tf.Dataset First batched tensorflow dataset to check. dataset_b : tf.Dataset Second batched tensorflow dataset to check. Return size : int The size of the dataset. compute_influence_vector_group( self , group : tf.Dataset) -> tf.Tensor \u00b6 Computes the influence function vector -- an estimation of the weights difference when removing the points -- of the whole group of points. Parameters group : tf.Dataset A batched TF dataset containing the group of points of which we wish to compute the influence of removal. Return influence_group : tf.Tensor A tensor containing one vector for the whole group. estimate_influence_values_group( self , group_train : tf.Dataset , group_to_evaluate : Optional[tf.Dataset] = None) -> tf.Tensor \u00b6 Computes Cook's distance of the whole group of points provided, giving measure of the influence that the group carries on the model's weights. Parameters group_train : tf.Dataset A batched TF dataset containing the group of points we wish to remove. group_to_evaluate : Optional[tf.Dataset] = None A batched TF dataset containing the group of points with respect to whom we wish to measure the influence of removing the training points. Return influence_values_group : tf.Tensor A tensor containing one influence value for the whole group.","title":"Second Order Influence Calculator"},{"location":"api/influence/second_order_influence_calculator/#second-order-influence-calculator","text":"View source | \ud83d\udcf0 Original Paper When working with groups of data, it can prove useful to take into account the pairwise interactions in terms of influence when leaving out large groups of data-points. Basu et al. have thus introduced a second-order formulation that takes these interactions into account: \\[ \\mathcal{I}^{(2)} (\\mathcal{U}, z_t) = \\nabla_\\theta \\ell (\\hat{\\theta}, z_t) \\left(\\mathcal{I}^{(1)} (\\mathcal{U}) + \\mathcal{I}' (\\mathcal{U})\\right) \\] \\[ \\mathcal{I}^{(1)} (\\mathcal{U}) = \\frac{1 - 2 p}{(1 - p)^2} \\frac{1}{|\\mathcal{S}|} H_{\\hat{\\theta}}^{-1} \\sum_{z \\in \\mathcal{U}} \\nabla_\\theta \\ell (\\hat{\\theta}, z) \\] \\[ \\mathcal{I}' (\\mathcal{U}) = \\frac{1}{(1 - p)^2} \\frac{1}{|S|^2} \\sum_{z \\in \\mathcal{U}} H_{\\hat{\\theta}}^{-1} \\nabla_\\theta^2 (\\hat{\\theta}, z) H_{\\hat{\\theta}}^{-1} \\sum_{z' \\in \\mathcal{U}} \\nabla_\\theta \\ell (\\hat{\\theta}, z') \\] As with the rest of the methods based on calculating inverse-hessian-vector products, an important part of the computations are carried out by objects from the class InverseHessianVectorProduct .","title":"Second Order Influence Calculator"},{"location":"api/influence/second_order_influence_calculator/#notebooks","text":"Using the second order influence calculator","title":"Notebooks"},{"location":"api/influence/second_order_influence_calculator/#SecondOrderInfluenceCalculator","text":"A class implementing the necessary methods to compute the different influence quantities (only for groups) using a second-order approximation, thus allowing us to take into account the pairwise interactions between points inside the group. For small groups of points, consider using the first order alternative if the computational cost is too high.","title":"SecondOrderInfluenceCalculator"},{"location":"api/representer_point_selection/rps_l2/","text":"Representer Point Selection - L2 \u00b6 View source | \ud83d\udcf0 Original Paper Using a completely different notion of influence than the techniques in modules deel.influenciae.influence and deel.influenciae.trac_in , this is the first method to use the representer point theorem for kernels to attribute an influence to data-points in the training dataset. In particular, it posits that the classification function can be approximated by: \\[ \\Phi (x_t, \\theta) = \\sum_i^n k (x_t, x_i, \\alpha_i) \\] \\[ \\alpha_i = - \\frac{1}{2 \\lambda n} \\frac{\\partial L (x_i, y_i, \\theta)}{\\partial \\Phi (x_i, \\theta)} \\] where \\(\\Phi\\) is the function that transforms the points \\(x_t\\) in the embedding of the network's last layer to the logits for the classification. However, this function must be learned with a strong \\(\\ell^2\\) regularization, and thus requires creating a surrogate model. In particular, it will be the \\(\\alpha_i\\) for the predicted label \\(j\\) the equivalent of the influence of the data-point. This method does not require us to compute inverse-hessian-vector products, so it can be computed with a certain efficiency once the surrogate model has been learned. Notebooks \u00b6 Using Representer Point Selection - L2 (RPS_L2) RepresenterPointL2 \u00b6 A class implementing a method to compute the influence of training points through the representer point theorem for kernels. __init__( self , model : keras.engine.training.Model , train_set : tf.Dataset , loss_function : Union[Callable[[tf.Tensor, tf.Tensor], tf.Tensor], keras.losses.Loss] , lambda_regularization : float , scaling_factor : float = 0.1 , epochs : int = 100 , layer_index : int = -2) \u00b6 Parameters model : keras.engine.training.Model A TF2 model that has already been trained train_set : tf.Dataset A batched TF dataset with the points with which the model was trained loss_function : Union[Callable[[tf.Tensor, tf.Tensor], tf.Tensor], keras.losses.Loss] The loss function with which the model was trained. This loss function MUST NOT be reduced. lambda_regularization : float The coefficient for the regularization of the surrogate last layer that needs to be trained for this method scaling_factor : float = 0.1 A float with the scaling factor for the SGD backtracking line-search optimizer for fitting the surrogate linear model epochs : int = 100 An integer for the amount of epochs to fit the linear model layer_index : int = -2 layer of the logits compute_influence_values( self , train_set : tf.Dataset , device : Optional[str] = None) -> tf.Dataset \u00b6 Compute the influence score for each sample of the provided (full or partial) model's training dataset. Parameters train_set : tf.Dataset A TF dataset with the (full or partial) model's training dataset. device : Optional[str] = None Device where the computation will be executed Return train_set : tf.Dataset A dataset containing the tuple: (batch of training samples, influence score) compute_influence_vector( self , train_set : tf.Dataset , save_influence_vector_ds_path : Optional[str] = None , device : Optional[str] = None) -> tf.Dataset \u00b6 Compute the influence vector for each sample of the provided (full or partial) model's training dataset. Parameters train_set : tf.Dataset A TF dataset with the (full or partial) model's training dataset. save_influence_vector_ds_path : Optional[str] = None The path to save or load the influence vector of the training dataset. If specified, load the dataset if it has already been computed, otherwise, compute the influence vector and then save it in the specified path. device : Optional[str] = None Device where the computation will be executed Return inf_vect_ds : tf.Dataset A dataset containing the tuple: (batch of training samples, influence vector) compute_top_k_from_training_dataset( self , train_set : tf.Dataset , k : int , order: deel.influenciae.utils.sorted_dict.ORDER = ) -> Tuple[tf.Tensor, tf.Tensor] \u00b6 Compute the k most influential data-points of the model's training dataset by computing Cook's distance for each point individually. Parameters train_set : tf.Dataset A TF dataset containing the points on which the model was trained. k : int An integer with the number of most important samples we wish to keep order : 2> Either ORDER.DESCENDING or ORDER.ASCENDING depending on if we wish to find the top-k or bottom-k samples, respectively. Return training_samples, influences_values : Tuple[tf.Tensor, tf.Tensor] A tuple of tensor. - training_samples: A tensor containing the k most influential samples of the training dataset for the model provided. - influences_values: The influence score corresponding to these k most influential samples. estimate_influence_values_in_batches( self , dataset_to_evaluate : tf.Dataset , train_set : tf.Dataset , influence_vector_in_cache: deel.influenciae.common.base_influence.CACHE = , load_influence_vector_path : Optional[str] = None , save_influence_vector_path : Optional[str] = None , save_influence_value_path : Optional[str] = None , device : Optional[str] = None) -> tf.Dataset \u00b6 Estimates the influence that each point in the provided training dataset has on each of the test points. This can provide some insights as to what makes the model predict a certain way for the given test points, and thus presents data-centric explanations. Parameters dataset_to_evaluate : tf.Dataset A TF dataset containing the test samples for which to compute the effect of removing each of the provided training points (individually). train_set : tf.Dataset A TF dataset containing the model's training dataset (partial or full). influence_vector_in_cache : 0> An enum indicating if intermediary values are to be cached (either in memory or on the disk) or not. Options include CACHE.MEMORY (0) for caching in memory, CACHE.DISK (1) for the disk and CACHE.NO_CACHE (2) for no optimization. load_influence_vector_path : Optional[str] = None The path to load the influence vectors (if they have already been calculated). save_influence_vector_path : Optional[str] = None The path to save the computed influence vector. save_influence_value_path : Optional[str] = None The path to save the computed influence values. device : Optional[str] = None Device where the computation will be executed Return influence_value_dataset : tf.Dataset A dataset containing the tuple: (samples_to_evaluate, dataset). - samples_to_evaluate: The batch of sample to evaluate. - dataset: Dataset containing tuples of batch of the training dataset and their influence score. predict_with_kernel( self , samples_to_evaluate : Tuple[tf.Tensor, ...]) -> tf.Tensor \u00b6 Uses the learned kernel to approximate the model's predictions on a group of samples. Parameters samples_to_evaluate : Tuple[tf.Tensor, ...] A single batch of tensors with the samples for which we wish to approximate the model's predictions Return predictions : tf.Tensor A tensor with an approximation of the model's predictions top_k( self , dataset_to_evaluate : tf.Dataset , train_set : tf.Dataset , k : int = 5 , nearest_neighbors : deel.influenciae.utils.nearest_neighbors.BaseNearestNeighbors = , influence_vector_in_cache: deel.influenciae.common.base_influence.CACHE = , load_influence_vector_ds_path : Optional[str] = None , save_influence_vector_ds_path : Optional[str] = None , save_top_k_ds_path : Optional[str] = None , order: deel.influenciae.utils.sorted_dict.ORDER = , d_type : tensorflow.python.framework.dtypes.DType = tf.float32 , device : Optional[str] = None) -> tf.Dataset \u00b6 Find the top-k closest elements for each element of dataset to evaluate in the training dataset The method will return a dataset containing a tuple of: (Top-k influence values for each sample to evaluate, Top-k training sample for each sample to evaluate) Parameters dataset_to_evaluate : tf.Dataset The dataset which contains the samples which will be compare to the training dataset train_set : tf.Dataset The dataset used to train the model. k : int = 5 the number of most influence samples to retain in training dataset nearest_neighbors : deel.influenciae.utils.nearest_neighbors.BaseNearestNeighbors = The nearest neighbor method. The default method is a linear search influence_vector_in_cache : 0> An enum indicating if intermediary values are to be cached (either in memory or on the disk) or not. Options include CACHE.MEMORY (0) for caching in memory, CACHE.DISK (1) for the disk and CACHE.NO_CACHE (2) for no optimization. load_influence_vector_ds_path : Optional[str] = None The path to load the influence vectors (if they have already been calculated). save_influence_vector_ds_path : Optional[str] = None The path to save the computed influence vector. save_top_k_ds_path : Optional[str] = None The path to save the result of the computation of the top-k elements order : 2> Either ORDER.DESCENDING or ORDER.ASCENDING depending on if we wish to find the top-k or bottom-k samples, respectively. d_type : tensorflow.python.framework.dtypes.DType = tf.float32 The data-type of the tensors. device : Optional[str] = None Device where the computation will be executed Return top_k_dataset : tf.Dataset A dataset containing the tuple (samples_to_evaluate, influence_values, training_samples). - samples_to_evaluate: Top-k samples to evaluate. - influence_values: Top-k influence values for each sample to evaluate. - training_samples: Top-k training sample for each sample to evaluate.","title":"Representer Point Selection - L2"},{"location":"api/representer_point_selection/rps_l2/#representer-point-selection-l2","text":"View source | \ud83d\udcf0 Original Paper Using a completely different notion of influence than the techniques in modules deel.influenciae.influence and deel.influenciae.trac_in , this is the first method to use the representer point theorem for kernels to attribute an influence to data-points in the training dataset. In particular, it posits that the classification function can be approximated by: \\[ \\Phi (x_t, \\theta) = \\sum_i^n k (x_t, x_i, \\alpha_i) \\] \\[ \\alpha_i = - \\frac{1}{2 \\lambda n} \\frac{\\partial L (x_i, y_i, \\theta)}{\\partial \\Phi (x_i, \\theta)} \\] where \\(\\Phi\\) is the function that transforms the points \\(x_t\\) in the embedding of the network's last layer to the logits for the classification. However, this function must be learned with a strong \\(\\ell^2\\) regularization, and thus requires creating a surrogate model. In particular, it will be the \\(\\alpha_i\\) for the predicted label \\(j\\) the equivalent of the influence of the data-point. This method does not require us to compute inverse-hessian-vector products, so it can be computed with a certain efficiency once the surrogate model has been learned.","title":"Representer Point Selection - L2"},{"location":"api/representer_point_selection/rps_l2/#notebooks","text":"Using Representer Point Selection - L2 (RPS_L2)","title":"Notebooks"},{"location":"api/representer_point_selection/rps_l2/#RepresenterPointL2","text":"A class implementing a method to compute the influence of training points through the representer point theorem for kernels.","title":"RepresenterPointL2"},{"location":"api/representer_point_selection/rps_lje/","text":"Representer Point Selection - Local Jacobian Expansion \u00b6 View source | \ud83d\udcf0 Original Paper Introduced as an improvement over Representer Point Selection - L2 , this technique trades the surrogate model for a local taylor expansion on the jacobian matrix, effectively allowing for the decomposition of the model's last layer into a kernel as an approximation. In short, it proposes the following formula for computing influence values: \\[ \\Theta_L^\\dagger \\phi (x_t) = \\sum_i \\alpha_i \\phi (x_i)^T \\phi (x_y) \\] \\[ \\alpha_i = \\Theta_L \\frac{1}{\\phi (x_i) \\, n} - \\frac{1}{n} H_{\\Theta_L}^{-1} \\frac{\\partial L (x_i, y_i, \\Theta)}{\\partial \\Theta_L \\phi (x_i)} \\] In particular, it will be the \\(\\alpha_i\\) for the predicted label \\(j\\) the equivalent of the influence of the data-point. As all the other methods based on computing inverse-hessian-vector products, it will be performing all these computations with the help of objects of the class InverseHessianVectorProduct , capable of doing so efficiently. Notebooks \u00b6 Using Representer Point Selection - Local Jacobian Expansion (RPS_LJE) RepresenterPointLJE \u00b6 Representer Point Selection via Local Jacobian Expansion for Post-hoc Classifier Explanation of Deep Neural Networks and Ensemble Models https://proceedings.neurips.cc/paper/2021/file/c460dc0f18fc309ac07306a4a55d2fd6-Paper.pdf __init__( self , influence_model : deel.influenciae.common.model_wrappers.InfluenceModel , dataset : tf.Dataset , ihvp_calculator_factory : deel.influenciae.common.ihvp_factory.InverseHessianVectorProductFactory , n_samples_for_hessian : Optional[int] = None , target_layer : Union[int, str] = -1 , shuffle_buffer_size : int = 10000) \u00b6 Parameters influence_model : deel.influenciae.common.model_wrappers.InfluenceModel The TF2.X model implementing the InfluenceModel interface. ihvp_calculator_factory : deel.influenciae.common.ihvp_factory.InverseHessianVectorProductFactory An InverseHessianVectorProductFactory for creating new instances of the InverseHessianVectorProduct class. n_samples_for_hessian : Optional[int] = None An integer for the amount of samples from the training dataset that will be used for the computation of the hessian matrix. If None, the whole dataset will be used. target_layer : Union[int, str] = -1 Either a string or an integer identifying the layer on which to compute the influence-related quantities. shuffle_buffer_size : int = 10000 An integer with the buffer size for the training set's shuffle operation. assert_compatible_datasets( dataset_a : tf.Dataset , dataset_b : tf.Dataset) -> int \u00b6 Assert that the datasets are compatible: that they contain the same number of points. Else, throw an error. Parameters dataset_a : tf.Dataset First batched tensorflow dataset to check. dataset_b : tf.Dataset Second batched tensorflow dataset to check. Return size : int The size of the dataset. compute_influence_values( self , train_set : tf.Dataset , device : Optional[str] = None) -> tf.Dataset \u00b6 Compute the influence score for each sample of the provided (full or partial) model's training dataset. Parameters train_set : tf.Dataset A TF dataset with the (full or partial) model's training dataset. device : Optional[str] = None Device where the computation will be executed Return train_set : tf.Dataset A dataset containing the tuple: (batch of training samples, influence score) compute_influence_vector( self , train_set : tf.Dataset , save_influence_vector_ds_path : Optional[str] = None , device : Optional[str] = None) -> tf.Dataset \u00b6 Compute the influence vector for each sample of the provided (full or partial) model's training dataset. Parameters train_set : tf.Dataset A TF dataset with the (full or partial) model's training dataset. save_influence_vector_ds_path : Optional[str] = None The path to save or load the influence vector of the training dataset. If specified, load the dataset if it has already been computed, otherwise, compute the influence vector and then save it in the specified path. device : Optional[str] = None Device where the computation will be executed Return inf_vect_ds : tf.Dataset A dataset containing the tuple: (batch of training samples, influence vector) compute_influence_vector_group( self , group : tf.Dataset) -> tf.Tensor \u00b6 Computes the influence function vector -- an estimation of the weights difference when removing the points -- of the whole group of points. Parameters group : tf.Dataset A batched TF dataset containing the group of points of which we wish to compute the influence of removal. Return influence_group : tf.Tensor A tensor containing one vector for the whole group. compute_top_k_from_training_dataset( self , train_set : tf.Dataset , k : int , order: deel.influenciae.utils.sorted_dict.ORDER = ) -> Tuple[tf.Tensor, tf.Tensor] \u00b6 Compute the k most influential data-points of the model's training dataset by computing Cook's distance for each point individually. Parameters train_set : tf.Dataset A TF dataset containing the points on which the model was trained. k : int An integer with the number of most important samples we wish to keep order : 2> Either ORDER.DESCENDING or ORDER.ASCENDING depending on if we wish to find the top-k or bottom-k samples, respectively. Return training_samples, influences_values : Tuple[tf.Tensor, tf.Tensor] A tuple of tensor. - training_samples: A tensor containing the k most influential samples of the training dataset for the model provided. - influences_values: The influence score corresponding to these k most influential samples. estimate_influence_values_group( self , group_train : tf.Dataset , group_to_evaluate : Optional[tf.Dataset] = None) -> tf.Tensor \u00b6 Computes Cook's distance of the whole group of points provided, giving measure of the influence that the group carries on the model's weights. Parameters group_train : tf.Dataset A batched TF dataset containing the group of points we wish to remove. group_to_evaluate : Optional[tf.Dataset] = None A batched TF dataset containing the group of points with respect to whom we wish to measure the influence of removing the training points. Return influence_values_group : tf.Tensor A tensor containing one influence value for the whole group. estimate_influence_values_in_batches( self , dataset_to_evaluate : tf.Dataset , train_set : tf.Dataset , influence_vector_in_cache: deel.influenciae.common.base_influence.CACHE = , load_influence_vector_path : Optional[str] = None , save_influence_vector_path : Optional[str] = None , save_influence_value_path : Optional[str] = None , device : Optional[str] = None) -> tf.Dataset \u00b6 Estimates the influence that each point in the provided training dataset has on each of the test points. This can provide some insights as to what makes the model predict a certain way for the given test points, and thus presents data-centric explanations. Parameters dataset_to_evaluate : tf.Dataset A TF dataset containing the test samples for which to compute the effect of removing each of the provided training points (individually). train_set : tf.Dataset A TF dataset containing the model's training dataset (partial or full). influence_vector_in_cache : 0> An enum indicating if intermediary values are to be cached (either in memory or on the disk) or not. Options include CACHE.MEMORY (0) for caching in memory, CACHE.DISK (1) for the disk and CACHE.NO_CACHE (2) for no optimization. load_influence_vector_path : Optional[str] = None The path to load the influence vectors (if they have already been calculated). save_influence_vector_path : Optional[str] = None The path to save the computed influence vector. save_influence_value_path : Optional[str] = None The path to save the computed influence values. device : Optional[str] = None Device where the computation will be executed Return influence_value_dataset : tf.Dataset A dataset containing the tuple: (samples_to_evaluate, dataset). - samples_to_evaluate: The batch of sample to evaluate. - dataset: Dataset containing tuples of batch of the training dataset and their influence score. top_k( self , dataset_to_evaluate : tf.Dataset , train_set : tf.Dataset , k : int = 5 , nearest_neighbors : deel.influenciae.utils.nearest_neighbors.BaseNearestNeighbors = , influence_vector_in_cache: deel.influenciae.common.base_influence.CACHE = , load_influence_vector_ds_path : Optional[str] = None , save_influence_vector_ds_path : Optional[str] = None , save_top_k_ds_path : Optional[str] = None , order: deel.influenciae.utils.sorted_dict.ORDER = , d_type : tensorflow.python.framework.dtypes.DType = tf.float32 , device : Optional[str] = None) -> tf.Dataset \u00b6 Find the top-k closest elements for each element of dataset to evaluate in the training dataset The method will return a dataset containing a tuple of: (Top-k influence values for each sample to evaluate, Top-k training sample for each sample to evaluate) Parameters dataset_to_evaluate : tf.Dataset The dataset which contains the samples which will be compare to the training dataset train_set : tf.Dataset The dataset used to train the model. k : int = 5 the number of most influence samples to retain in training dataset nearest_neighbors : deel.influenciae.utils.nearest_neighbors.BaseNearestNeighbors = The nearest neighbor method. The default method is a linear search influence_vector_in_cache : 0> An enum indicating if intermediary values are to be cached (either in memory or on the disk) or not. Options include CACHE.MEMORY (0) for caching in memory, CACHE.DISK (1) for the disk and CACHE.NO_CACHE (2) for no optimization. load_influence_vector_ds_path : Optional[str] = None The path to load the influence vectors (if they have already been calculated). save_influence_vector_ds_path : Optional[str] = None The path to save the computed influence vector. save_top_k_ds_path : Optional[str] = None The path to save the result of the computation of the top-k elements order : 2> Either ORDER.DESCENDING or ORDER.ASCENDING depending on if we wish to find the top-k or bottom-k samples, respectively. d_type : tensorflow.python.framework.dtypes.DType = tf.float32 The data-type of the tensors. device : Optional[str] = None Device where the computation will be executed Return top_k_dataset : tf.Dataset A dataset containing the tuple (samples_to_evaluate, influence_values, training_samples). - samples_to_evaluate: Top-k samples to evaluate. - influence_values: Top-k influence values for each sample to evaluate. - training_samples: Top-k training sample for each sample to evaluate.","title":"Representer Point Selection - LJE"},{"location":"api/representer_point_selection/rps_lje/#representer-point-selection-local-jacobian-expansion","text":"View source | \ud83d\udcf0 Original Paper Introduced as an improvement over Representer Point Selection - L2 , this technique trades the surrogate model for a local taylor expansion on the jacobian matrix, effectively allowing for the decomposition of the model's last layer into a kernel as an approximation. In short, it proposes the following formula for computing influence values: \\[ \\Theta_L^\\dagger \\phi (x_t) = \\sum_i \\alpha_i \\phi (x_i)^T \\phi (x_y) \\] \\[ \\alpha_i = \\Theta_L \\frac{1}{\\phi (x_i) \\, n} - \\frac{1}{n} H_{\\Theta_L}^{-1} \\frac{\\partial L (x_i, y_i, \\Theta)}{\\partial \\Theta_L \\phi (x_i)} \\] In particular, it will be the \\(\\alpha_i\\) for the predicted label \\(j\\) the equivalent of the influence of the data-point. As all the other methods based on computing inverse-hessian-vector products, it will be performing all these computations with the help of objects of the class InverseHessianVectorProduct , capable of doing so efficiently.","title":"Representer Point Selection - Local Jacobian Expansion"},{"location":"api/representer_point_selection/rps_lje/#notebooks","text":"Using Representer Point Selection - Local Jacobian Expansion (RPS_LJE)","title":"Notebooks"},{"location":"api/representer_point_selection/rps_lje/#RepresenterPointLJE","text":"Representer Point Selection via Local Jacobian Expansion for Post-hoc Classifier Explanation of Deep Neural Networks and Ensemble Models https://proceedings.neurips.cc/paper/2021/file/c460dc0f18fc309ac07306a4a55d2fd6-Paper.pdf","title":"RepresenterPointLJE"}]}