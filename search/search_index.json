{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Explore Influenciae docs \u00bb <p>Influenciae is a Python toolkit dedicated to computing influence values for the discovery of potentially problematic samples in a dataset and the generation of data-centric explanations for deep learning models. In this library based on Tensorflow, we gather state-of-the-art methods for estimating the importance of training samples and their influence on test data-points for validating the quality of datasets and of the models trained on them.</p>"},{"location":"#tutorials","title":"\ud83d\udd25 Tutorials","text":"<p>We propose some hands-on tutorials to get familiar with the library and it's API:</p> <ul> <li>Getting Started </li> <li>Benchmarking with Mislabeled sample detection </li> <li>Using the first order influence calculator </li> <li>Using the second order influence calculator </li> <li>Using TracIn </li> <li>Using Representer Point Selection - L2 (RPS_L2) </li> <li>Using Representer Point Selection - Local Jacobian Expansion (RPS_LJE) </li> <li>Using Arnoldi Influence Calculator </li> <li>Using Boundary-based Influence </li> </ul>"},{"location":"#quick-start","title":"\ud83d\ude80 Quick Start","text":"<p>Influenciae requires a version of python 3.7 or higher and several libraries, including Tensorflow and Numpy. Installation can be done using Pypi:</p> <pre><code>pip install influenciae\n</code></pre> <p>Once Influenciae is installed, there are two major applications for the different modules (that all follow the same API). So, except for group-specific functions that are only available on the <code>influence</code> module, all the classes are able to compute self-influence values, the influence with one point w.r.t. another, as well as find the top-k samples for both of these situations.</p>"},{"location":"#discovering-influential-examples","title":"Discovering influential examples","text":"<p>Particularly useful when validating datasets, influence functions (and related notions) allow for gaining an insight into what samples the models thinks to be \"important\". For this, the training dataset and a trained model are needed.</p> <pre><code>from deel.influenciae.common import InfluenceModel, ExactIHVP\nfrom deel.influenciae.influence import FirstOrderInfluenceCalculator\nfrom deel.influenciae.utils import ORDER\n\n# load the model, the training loss (without reduction) and the training data (with the labels and in a batched TF dataset)\n\ninfluence_model = InfluenceModel(model, target_layer, loss_function)\nihvp_calculator = ExactIHVP(influence_model, train_dataset)\ninfluence_calculator = FirstOrderInfluenceCalculator(influence_model, train_dataset, ihvp_calculator)\ndata_and_influence_dataset = influence_calculator.compute_influence_values(train_dataset)\n# or influence_calculator.compute_top_k_from_training_dataset(train_dataset, k_samples, ORDER.DESCENDING) when the\n# dataset is too large\n</code></pre> <p>This is also explained more in depth in the Getting Started tutotial </p>"},{"location":"#explaining-neural-networks-through-their-training-data","title":"Explaining neural networks through their training data","text":"<p>Another application is to explain some model's predictions by looking on which training samples they are based on. Again, the training dataset, the model and the samples we wish to explain are needed.</p> <pre><code>from deel.influenciae.common import InfluenceModel, ExactIHVP\nfrom deel.influenciae.influence import FirstOrderInfluenceCalculator\nfrom deel.influenciae.utils import ORDER\n\n# load the model, the training loss (without reduction), the training data and\n# the data to explain (with the labels and in batched a TF dataset)\n\ninfluence_model = InfluenceModel(model, target_layer, loss_function)\nihvp_calculator = ExactIHVP(influence_model, train_dataset)\ninfluence_calculator = FirstOrderInfluenceCalculator(influence_model, train_dataset, ihvp_calculator)\ndata_and_influence_dataset = influence_calculator.estimate_influence_values_in_batches(samples_to_explain, train_dataset)\n# or influence_calculator.top_k(samples_to_explain, train_dataset, k_samples, ORDER.DESCENDING) when the\n# dataset is too large\n</code></pre> <p>This is also explained more in depth in the Getting Started tutorial </p>"},{"location":"#determining-the-influence-of-groups-of-samples","title":"Determining the influence of groups of samples","text":"<p>The previous examples use notions of influence that are applied individually to each data-point, but it is possible to extend this to groups. That is, answer the question of what would a model look like if it hadn't seen a whole group of data-points during training, for example. This can be computed namely using the <code>FirstOrderInfluenceCalculator</code> and <code>SecondOrderInfluenceCalculator</code>, for implementations where pairwise interactions between each of the data-points are not taken into account and do, respectively.</p> <p>For obtaining the groups' influence:</p> <pre><code>from deel.influenciae.common import InfluenceModel, ExactIHVP\nfrom deel.influenciae.influence import SecondOrderInfluenceCalculator\n\n# load the model, the training loss (without reduction), the training data and\n# the data to explain (with the labels and in a batched TF dataset)\n\ninfluence_model = InfluenceModel(model, target_layer, loss_function)\nihvp_calculator = ExactIHVP(influence_model, train_dataset)\ninfluence_calculator = SecondOrderInfluenceCalculator(influence_model, train_dataset, ihvp_calculator)  # or FirstOrderInfluenceCalculator\ndata_and_influence_dataset = influence_calculator.estimate_influence_values_group(groups_train, groups_to_explain)\n</code></pre> <p>For the data-centric explanations:</p> <pre><code>from deel.influenciae.common import InfluenceModel, ExactIHVP\nfrom deel.influenciae.influence import SecondOrderInfluenceCalculator\n\n# load the model, the training loss (without reduction), the training data and\n# the data to explain (with the labels and in a batched TF dataset)\n\ninfluence_model = InfluenceModel(model, target_layer, loss_function)\nihvp_calculator = ExactIHVP(influence_model, train_dataset)\ninfluence_calculator = SecondOrderInfluenceCalculator(influence_model, train_dataset, ihvp_calculator)  # or FirstOrderInfluenceCalculator\ndata_and_influence_dataset = influence_calculator.estimate_influence_values_group(groups_train)\n</code></pre>"},{"location":"#whats-included","title":"\ud83d\udce6 What's Included","text":"<p>All the influence calculation methods work on Tensorflow models trained for any sort of task and on any type of data. Visualization functionality is implemented for image datasets only (for the moment).</p> Influence Method Source Tutorial Influence Functions Paper RelatIF Paper Influence Functions  (first order, groups) Paper Influence Functions  (second order, groups) Paper Arnoldi iteration (Scaling Up Influence Functions) Paper Representer Point Selection  (L2) Paper Representer Point Selection  (Local Jacobian Expansion) Paper Trac-In Paper Boundary-based influence -- - Using Boundary-based Influence"},{"location":"#see-also","title":"\ud83d\udc40 See Also","text":"<p>This library proposes implementations of some of the different popular ways of calculating the influence of data-points on TF, but there are also other ones using other frameworks. </p> <p>Some other tools for efficiently computing influence functions.</p> <ul> <li>Scaling Up Influence Functions a Python library using JAX implementing a scalable algorithm for computing influence functions.</li> <li>FastIF: Scalable Influence Functions for Efficient Model Interpretation and Debugging a Python library using PyTorch implementing another scalable algorithm for computing influence functions.</li> </ul> <p>More from the DEEL project:</p> <ul> <li>Xplique a Python library exclusively dedicated to explaining neural networks.</li> <li>deel-lip a Python library for training k-Lipschitz neural networks on TF.</li> <li>deel-torchlip a Python library for training k-Lipschitz neural networks on PyTorch.</li> <li>DEEL White paper a summary of the DEEL team on the challenges of certifiable AI and the role of data quality, representativity and explainability for this purpose.</li> </ul>"},{"location":"#acknowledgments","title":"\ud83d\ude4f Acknowledgments","text":"<p> This project received funding from the French \u201dInvesting for the Future \u2013 PIA3\u201d program within the Artificial and Natural Intelligence Toulouse Institute (ANITI). The authors gratefully acknowledge the support of the  DEEL  project.</p>"},{"location":"#creators","title":"\ud83d\udc68\u200d\ud83c\udf93 Creators","text":"<p>This library was first created as a research tool by Agustin Martin PICARD in the context of the DEEL project with the help of David Vigouroux and Thomas FEL. Later on, Lucas Hervier joined the team to transform (at least attempt) the code base as a practical user-(almost)-friendly and efficient tool.</p>"},{"location":"#citation","title":"\ud83d\uddde\ufe0f Citation","text":"<p>If you use Influenciae as part of your workflow in a scientific publication, please consider citing the \ud83d\uddde\ufe0f official paper:</p> <pre><code>@unpublished{picard:hal-04284178,\n  TITLE = {Influenci\\{ae}: A library for tracing the influence back to the data-points},\n  AUTHOR = {Picard, Agustin Martin and Hervier, Lucas and Fel, Thomas and Vigouroux, David},\n  URL = {https://hal.science/hal-04284178},\n  NOTE = {working paper or preprint},\n  YEAR = {2023},\n  MONTH = Nov,\n  KEYWORDS = {Data-centric ai ; XAI ; Explainability ; Influence Functions ; Open-source toolbox},\n  PDF = {https://hal.science/hal-04284178/file/ms.pdf},\n  HAL_ID = {hal-04284178},\n  HAL_VERSION = {v1},\n}\n</code></pre>"},{"location":"#license","title":"\ud83d\udcdd License","text":"<p>The package is released under  MIT license.</p>"},{"location":"api/benchmarks/","title":"Benchmarks","text":"<p>This module allows users to easily and objectively compare the performance of the different techniques that have been implemented in this library, or new ones that may come up. The benchmark is based on a widespread test used in the literature to measure the different methods' capacity to find problematic data-points: train a model on a dataset that has a certain percentage of mislabeled data-points (that are already known for validation purposes), and use the influence values to rank them and see what percentage of the whole dataset must be checked to find all the mislabeled points when ordered that way. In particular, a good metric for this is the AUC (area under the curve) of the ROC curve of this operation.</p> <p>Additionally, as we are working with stochastic algorithms, we need to make sure that the scores we obtain have a statistical value. This means that this operation should be repeated over multiple seeds to make sure that the method really does perform better than others. For this reason, a training procedure must be created so as to be able to automatically repeat the process of training the model on noisy versions of the original dataset.</p> <p>In particular, this module includes interfaces for training procedures and mislabeled sample detection benchmarking, and an implementation on the popular CIFAR10 image-classification dataset.</p>"},{"location":"api/benchmarks/#notebooks","title":"Notebooks","text":"<ul> <li>Benchmarking with Mislabeled sample detection</li> </ul>"},{"location":"api/tracin/","title":"TracIn","text":"<p>View source | \ud83d\udcf0 Original Paper</p> <p>This method proposes an alternative for estimating influence without the need for expensive inverse hessian-vector product computations, but requiring information that is only available at train time. It leverages the fundamental theorem of calculus to estimate the influence of the training points by looking at how the loss at that point evolves at different model checkpoints. Concretely, the influence will take the following for:</p> \\[ \\mathcal{I} (z, z') = \\sum_i^k \\eta_i \\nabla_\\theta \\ell (\\theta_{t_i}, z) \\cdot \\nabla_\\theta \\ell (\\theta_{t_i}, z') \\] <p>where \\(\\theta_{t_i}\\) are the model's weights at epoch \\(t_i\\) and \\(\\eta_i\\) is the learning rate at that same epoch.</p> <p>Just like RPS-L2, this method does not need an instance of the <code>InverseHessianVectorProduct</code> class, but does require to provide some of the model's checkpoints and the learning rates at each of them.</p>"},{"location":"api/tracin/#notebooks","title":"Notebooks","text":"<ul> <li>Using TracIn</li> </ul>"},{"location":"api/tracin/#TracIn","title":"<code>TracIn</code>","text":"<p>A class implementing an influence score based on TracIn method proposed in https://arxiv.org/pdf/2002.08484.pdf </p>"},{"location":"api/tracin/#__init__","title":"<code>__init__(self,  models:  List[deel.influenciae.common.model_wrappers.InfluenceModel],  learning_rates:  Union[float, List[float]])</code>","text":"<p>Parameters</p> <ul> <li> <p>models            : List[deel.influenciae.common.model_wrappers.InfluenceModel] </p> <ul> <li><p> A list of TF2.X models implementing the InfluenceModel interface at different steps (epochs) of the training</p> </li> </ul> </li> <li> <p>learning_rates            : Union[float, List[float]] </p> <ul> <li><p> Learning rate or list of learning rates used during the training.</p><p> If learning_rates is a list, it should have the same size as the amount of models</p> </li> </ul> </li> </ul>"},{"location":"api/tracin/#compute_influence_values","title":"<code>compute_influence_values(self,  train_set:  tf.Dataset,  device:  Optional[str] = None) -&gt; tf.Dataset</code>","text":"<p>Compute the influence score for each sample of the provided (full or partial) model's training dataset. </p> <p>Parameters</p> <ul> <li> <p>train_set            : tf.Dataset </p> <ul> <li><p> A TF dataset with the (full or partial) model's training dataset.</p> </li> </ul> </li> <li> <p>device            : Optional[str] = None </p> <ul> <li><p> Device where the computation will be executed</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>train_set            : tf.Dataset </p> <ul> <li><p> A dataset containing the tuple: (batch of training samples, influence score)</p> </li> </ul> </li> </ul> <p></p>"},{"location":"api/tracin/#compute_influence_vector","title":"<code>compute_influence_vector(self,  train_set:  tf.Dataset,  save_influence_vector_ds_path:  Optional[str] = None,  device:  Optional[str] = None) -&gt; tf.Dataset</code>","text":"<p>Compute the influence vector for each sample of the provided (full or partial) model's training dataset. </p> <p>Parameters</p> <ul> <li> <p>train_set            : tf.Dataset </p> <ul> <li><p> A TF dataset with the (full or partial) model's training dataset.</p> </li> </ul> </li> <li> <p>save_influence_vector_ds_path            : Optional[str] = None </p> <ul> <li><p> The path to save or load the influence vector of the training dataset. If specified, load the dataset if it has already been computed, otherwise, compute the influence vector and then save it in the specified path.</p> </li> </ul> </li> <li> <p>device            : Optional[str] = None </p> <ul> <li><p> Device where the computation will be executed</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>inf_vect_ds            : tf.Dataset </p> <ul> <li><p> A dataset containing the tuple: (batch of training samples, influence vector)</p> </li> </ul> </li> </ul> <p></p>"},{"location":"api/tracin/#compute_top_k_from_training_dataset","title":"<code>compute_top_k_from_training_dataset(self,  train_set:  tf.Dataset,  k:  int,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 order: deel.influenciae.utils.sorted_dict.ORDER = ) -&gt; Tuple[tf.Tensor, tf.Tensor] <p>Compute the k most influential data-points of the model's training dataset by computing Cook's distance for each point individually. </p> <p>Parameters</p> <ul> <li> <p>train_set            : tf.Dataset </p> <ul> <li><p> A TF dataset containing the points on which the model was trained.</p> </li> </ul> </li> <li> <p>k            : int </p> <ul> <li><p> An integer with the number of most important samples we wish to keep</p> </li> </ul> </li> <li> <p>order            : 2&gt; </p> <ul> <li><p> Either ORDER.DESCENDING or ORDER.ASCENDING depending on if we wish to find the top-k or bottom-k samples, respectively.</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>training_samples, influences_values            : Tuple[tf.Tensor, tf.Tensor] </p> <ul> <li><p> A tuple of tensor.</p><p> - training_samples: A tensor containing the k most influential samples of the training dataset for the model provided.</p><p> - influences_values: The influence score corresponding to these k most influential samples.</p> </li> </ul> </li> </ul> <p></p>","text":""},{"location":"api/tracin/#estimate_influence_values_in_batches","title":"<code>estimate_influence_values_in_batches(self,  dataset_to_evaluate:  tf.Dataset,  train_set:  tf.Dataset,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 influence_vector_in_cache: deel.influenciae.common.base_influence.CACHE = ,  load_influence_vector_path:  Optional[str] = None,  save_influence_vector_path:  Optional[str] = None,  save_influence_value_path:  Optional[str] = None,  device:  Optional[str] = None) -&gt; tf.Dataset <p>Estimates the influence that each point in the provided training dataset has on each of the test points. This can provide some insights as to what makes the model predict a certain way for the given test points, and thus presents data-centric explanations. </p> <p>Parameters</p> <ul> <li> <p>dataset_to_evaluate            : tf.Dataset </p> <ul> <li><p> A TF dataset containing the test samples for which to compute the effect of removing each of the provided training points (individually).</p> </li> </ul> </li> <li> <p>train_set            : tf.Dataset </p> <ul> <li><p> A TF dataset containing the model's training dataset (partial or full).</p> </li> </ul> </li> <li> <p>influence_vector_in_cache            : 0&gt; </p> <ul> <li><p> An enum indicating if intermediary values are to be cached (either in memory or on the disk) or not.</p><p> Options include CACHE.MEMORY (0) for caching in memory, CACHE.DISK (1) for the disk and CACHE.NO_CACHE (2) for no optimization.</p> </li> </ul> </li> <li> <p>load_influence_vector_path            : Optional[str] = None </p> <ul> <li><p> The path to load the influence vectors (if they have already been calculated).</p> </li> </ul> </li> <li> <p>save_influence_vector_path            : Optional[str] = None </p> <ul> <li><p> The path to save the computed influence vector.</p> </li> </ul> </li> <li> <p>save_influence_value_path            : Optional[str] = None </p> <ul> <li><p> The path to save the computed influence values.</p> </li> </ul> </li> <li> <p>device            : Optional[str] = None </p> <ul> <li><p> Device where the computation will be executed</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>influence_value_dataset            : tf.Dataset </p> <ul> <li><p> A dataset containing the tuple: (samples_to_evaluate, dataset).</p><p>  - samples_to_evaluate: The batch of sample to evaluate.</p><p> - dataset: Dataset containing tuples of batch of the training dataset and their influence score.</p> </li> </ul> </li> </ul> <p></p>","text":""},{"location":"api/tracin/#top_k","title":"<code>top_k(self,  dataset_to_evaluate:  tf.Dataset,  train_set:  tf.Dataset,  k:  int = 5,  nearest_neighbors:  deel.influenciae.utils.nearest_neighbors.BaseNearestNeighbors = ,\u00a0\u00a0\u00a0\u00a0\u00a0 influence_vector_in_cache: deel.influenciae.common.base_influence.CACHE = ,  load_influence_vector_ds_path:  Optional[str] = None,  save_influence_vector_ds_path:  Optional[str] = None,  save_top_k_ds_path:  Optional[str] = None,\u00a0\u00a0\u00a0\u00a0\u00a0 order: deel.influenciae.utils.sorted_dict.ORDER = ,  d_type:  tensorflow.python.framework.dtypes.DType = tf.float32,  device:  Optional[str] = None) -&gt; tf.Dataset <p>Find the top-k closest elements for each element of dataset to evaluate in the training dataset The method will return a dataset containing a tuple of:     (Top-k influence values for each sample to evaluate, Top-k training sample for each sample to evaluate) </p> <p>Parameters</p> <ul> <li> <p>dataset_to_evaluate            : tf.Dataset </p> <ul> <li><p> The dataset which contains the samples which will be compare to the training dataset</p> </li> </ul> </li> <li> <p>train_set            : tf.Dataset </p> <ul> <li><p> The dataset used to train the model.</p> </li> </ul> </li> <li> <p>k            : int = 5 </p> <ul> <li><p> the number of most influence samples to retain in training dataset</p> </li> </ul> </li> <li> <p>nearest_neighbors            : deel.influenciae.utils.nearest_neighbors.BaseNearestNeighbors =   <ul> <li><p> The nearest neighbor method. The default method is a linear search</p> </li> </ul>  <li> <p>influence_vector_in_cache            : 0&gt; </p> <ul> <li><p> An enum indicating if intermediary values are to be cached (either in memory or on the disk) or not.</p><p> Options include CACHE.MEMORY (0) for caching in memory, CACHE.DISK (1) for the disk and CACHE.NO_CACHE (2) for no optimization.</p> </li> </ul> </li> <li> <p>load_influence_vector_ds_path            : Optional[str] = None </p> <ul> <li><p> The path to load the influence vectors (if they have already been calculated).</p> </li> </ul> </li> <li> <p>save_influence_vector_ds_path            : Optional[str] = None </p> <ul> <li><p> The path to save the computed influence vector.</p> </li> </ul> </li> <li> <p>save_top_k_ds_path            : Optional[str] = None </p> <ul> <li><p> The path to save the result of the computation of the top-k elements</p> </li> </ul> </li> <li> <p>order            : 2&gt; </p> <ul> <li><p> Either ORDER.DESCENDING or ORDER.ASCENDING depending on if we wish to find the top-k or bottom-k samples, respectively.</p> </li> </ul> </li> <li> <p>d_type            : tensorflow.python.framework.dtypes.DType = tf.float32 </p> <ul> <li><p> The data-type of the tensors.</p> </li> </ul> </li> <li> <p>device            : Optional[str] = None </p> <ul> <li><p> Device where the computation will be executed</p> </li> </ul> </li>  <p>Return</p> <ul> <li> <p>top_k_dataset            : tf.Dataset </p> <ul> <li><p> A dataset containing the tuple (samples_to_evaluate, influence_values, training_samples).</p><p>  - samples_to_evaluate: Top-k samples to evaluate.</p><p> - influence_values: Top-k influence values for each sample to evaluate.</p><p> - training_samples: Top-k training sample for each sample to evaluate.</p> </li> </ul> </li> </ul> <p></p>","text":""},{"location":"api/Boundary-based/sample_boundary/","title":"Sample boundary","text":"<p>View source</p> <p>For a completely different notion of influence or importance of data-points, we propose to measure the distance that separates each data-point from the decision boundary, and assign a higher influence score to the elements that are  closest to the decision boundary. It would make sense for these examples to be the most influential, as if they weren't there, the model would have placed the decision boundary elsewhere.</p> <p>In particular, we define the influence score as follows:</p> <p>$$ \\mathcal{I}{SB} (z) = - \\lVert z - z \\rVert^2 \\, , $$ where \\(z\\) is the data-point under study and \\(z_{adv}\\) is the adversarial example with the lowest possible budget  and obtained through the DeepFool method.</p> <p>This technique is based on a simple idea we had, and as such, there's no paper associated to it. We decided to include it because it seems that its performance is less dependent on the choice of model and training schedule and still obtains acceptable results on our mislabeled point detection benchmark.</p>"},{"location":"api/Boundary-based/sample_boundary/#notebooks","title":"Notebooks","text":"<ul> <li>Using Boundary-based Influence </li> </ul>"},{"location":"api/Boundary-based/sample_boundary/#SampleBoundaryCalculator","title":"<code>SampleBoundaryCalculator</code>","text":"<p>A class implementing an influence score based on the distance of a sample to the boundary of the classifier. The distance to the boundary is estimated using the deep fool method. [https://arxiv.org/abs/1511.04599] </p>"},{"location":"api/Boundary-based/sample_boundary/#__init__","title":"<code>__init__(self,  model:  keras.engine.training.Model,  step_nbr:  int = 100,  eps:  float = 1e-06)</code>","text":"<p>Parameters</p> <ul> <li> <p>model            : keras.engine.training.Model </p> <ul> <li><p> A TF2 model that has already been trained</p> </li> </ul> </li> <li> <p>step_nbr            : int = 100 </p> <ul> <li><p> Number of the iterations to find the closest adversarial problem</p> </li> </ul> </li> <li> <p>eps            : float = 1e-06 </p> <ul> <li><p> Difference between two logits to assume that they have the same values</p> </li> </ul> </li> </ul>"},{"location":"api/Boundary-based/sample_boundary/#compute_influence_values","title":"<code>compute_influence_values(self,  train_set:  tf.Dataset,  device:  Optional[str] = None) -&gt; tf.Dataset</code>","text":"<p>Compute the influence score for each sample of the provided (full or partial) model's training dataset. </p> <p>Parameters</p> <ul> <li> <p>train_set            : tf.Dataset </p> <ul> <li><p> A TF dataset with the (full or partial) model's training dataset.</p> </li> </ul> </li> <li> <p>device            : Optional[str] = None </p> <ul> <li><p> Device where the computation will be executed</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>train_set            : tf.Dataset </p> <ul> <li><p> A dataset containing the tuple: (batch of training samples, influence score)</p> </li> </ul> </li> </ul> <p></p>"},{"location":"api/Boundary-based/sample_boundary/#compute_top_k_from_training_dataset","title":"<code>compute_top_k_from_training_dataset(self,  train_set:  tf.Dataset,  k:  int,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 order: deel.influenciae.utils.sorted_dict.ORDER = ) -&gt; Tuple[tf.Tensor, tf.Tensor] <p>Compute the k most influential data-points of the model's training dataset by computing Cook's distance for each point individually. </p> <p>Parameters</p> <ul> <li> <p>train_set            : tf.Dataset </p> <ul> <li><p> A TF dataset containing the points on which the model was trained.</p> </li> </ul> </li> <li> <p>k            : int </p> <ul> <li><p> An integer with the number of most important samples we wish to keep</p> </li> </ul> </li> <li> <p>order            : 2&gt; </p> <ul> <li><p> Either ORDER.DESCENDING or ORDER.ASCENDING depending on if we wish to find the top-k or bottom-k samples, respectively.</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>training_samples, influences_values            : Tuple[tf.Tensor, tf.Tensor] </p> <ul> <li><p> A tuple of tensor.</p><p> - training_samples: A tensor containing the k most influential samples of the training dataset for the model provided.</p><p> - influences_values: The influence score corresponding to these k most influential samples.</p> </li> </ul> </li> </ul> <p></p>","text":""},{"location":"api/Boundary-based/weights_boundary/","title":"Weights boundary","text":"<p>View source</p> <p>For a completely different notion of influence or importance of data-points, we propose to measure the budget (measured through an \\(\\el^2\\) metric) needed to minimally perturb the model's weights such that the data-point under study gets misclassified. Ideally, it would make sense for more influential images to need a smaller budget (i.e. a smaller change on the model) to make the model change its prediction on them.</p> <p>In particular, we define the influence score as follows:</p> <p>$$ \\mathcal{I}{WB} (z) = - \\lVert w - w \\rVert^2 \\, , $$ where \\(w\\) is the model's weights and \\(w_{adv}\\) is the perturbed model with the lowest possible budget and  obtained through an adaptation of the DeepFool method.</p> <p>This technique is based on a simple idea we had, and as such, there's no paper associated to it. We decided to include it because it seems that its performance is less dependent on the choice of model and training schedule and still obtains acceptable results on our mislabeled point detection benchmark.</p>"},{"location":"api/Boundary-based/weights_boundary/#notebooks","title":"Notebooks","text":"<ul> <li>Using Boundary-based Influence </li> </ul>"},{"location":"api/Boundary-based/weights_boundary/#WeightsBoundaryCalculator","title":"<code>WeightsBoundaryCalculator</code>","text":"<p>A class implementing an influence score based on the distance of a sample to the boundary of its classifier. The distance to the boundary is estimated by deforming the boundary of the model to move a given sample to the closest adversarial class. To compute this distance, the deep fool method is used on the weights of the model (deep fool originally compute the distance on the sample space). [https://arxiv.org/abs/1511.04599] </p>"},{"location":"api/Boundary-based/weights_boundary/#__init__","title":"<code>__init__(self,  model:  keras.engine.training.Model,  step_nbr:  int = 100,  norm_type:  int = 2,  eps:  float = 1e-06)</code>","text":"<p>Parameters</p> <ul> <li> <p>model            : keras.engine.training.Model </p> <ul> <li><p> A TF2 model that has already been trained</p> </li> </ul> </li> <li> <p>step_nbr            : int = 100 </p> <ul> <li><p> Number of the iterations to find the closest adversarial problem</p> </li> </ul> </li> <li> <p>norm_type            : int = 2 </p> <ul> <li><p> The distance norm used to compute the distance to the boundary</p> </li> </ul> </li> <li> <p>eps            : float = 1e-06 </p> <ul> <li><p> Difference between two logits to assume that the logits have the same values</p> </li> </ul> </li> </ul>"},{"location":"api/Boundary-based/weights_boundary/#compute_influence_values","title":"<code>compute_influence_values(self,  train_set:  tf.Dataset,  device:  Optional[str] = None) -&gt; tf.Dataset</code>","text":"<p>Compute the influence score for each sample of the provided (full or partial) model's training dataset. </p> <p>Parameters</p> <ul> <li> <p>train_set            : tf.Dataset </p> <ul> <li><p> A TF dataset with the (full or partial) model's training dataset.</p> </li> </ul> </li> <li> <p>device            : Optional[str] = None </p> <ul> <li><p> Device where the computation will be executed</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>train_set            : tf.Dataset </p> <ul> <li><p> A dataset containing the tuple: (batch of training samples, influence score)</p> </li> </ul> </li> </ul> <p></p>"},{"location":"api/Boundary-based/weights_boundary/#compute_top_k_from_training_dataset","title":"<code>compute_top_k_from_training_dataset(self,  train_set:  tf.Dataset,  k:  int,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 order: deel.influenciae.utils.sorted_dict.ORDER = ) -&gt; Tuple[tf.Tensor, tf.Tensor] <p>Compute the k most influential data-points of the model's training dataset by computing Cook's distance for each point individually. </p> <p>Parameters</p> <ul> <li> <p>train_set            : tf.Dataset </p> <ul> <li><p> A TF dataset containing the points on which the model was trained.</p> </li> </ul> </li> <li> <p>k            : int </p> <ul> <li><p> An integer with the number of most important samples we wish to keep</p> </li> </ul> </li> <li> <p>order            : 2&gt; </p> <ul> <li><p> Either ORDER.DESCENDING or ORDER.ASCENDING depending on if we wish to find the top-k or bottom-k samples, respectively.</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>training_samples, influences_values            : Tuple[tf.Tensor, tf.Tensor] </p> <ul> <li><p> A tuple of tensor.</p><p> - training_samples: A tensor containing the k most influential samples of the training dataset for the model provided.</p><p> - influences_values: The influence score corresponding to these k most influential samples.</p> </li> </ul> </li> </ul> <p></p>","text":""},{"location":"api/influence/arnoldi/","title":"Arnoldi Influence Calculator","text":"<p> View source | \ud83d\udcf0 Paper</p> <p>This class implements the method introduced in Scaling Up Influence Functions, Schioppa et al. at AAAI 2022. It proposes a series of memory and computational optimizations based on the Arnoldi iteration for speeding up inverse hessian calculators, allowing the authors to approximately compute influence functions on whole  large vision models (going up to a ViT-L with 300M parameters).</p> <p>In essence, the optimizations can be summarized as follows: - build an orthonormal basis for the Krylov subspaces of a random vector (in the desired dimensionality). - find the eigenvalues and eigenvectors of the restriction of the Hessian matrix in that restricted subspace. - keep only the \\(k\\) largest eigenvalues and their corresponding eigenvectors, and create a projection matrix \\(G\\) into this space. - use forward-over-backward auto-differentiation to directly compute the JVPs in this reduced space.</p> <p>Due to the specificity of these optimizations, the inverse hessian vector product operation is implemented inside the class, and thus, doesn't require an additional separate IHVP object. In addition, it can only be applied to individual points for the moment.</p>"},{"location":"api/influence/arnoldi/#notebooks","title":"Notebooks","text":"<ul> <li>Using Arnoldi Influence Calculator</li> </ul>"},{"location":"api/influence/arnoldi/#ArnoldiInfluenceCalculator","title":"<code>ArnoldiInfluenceCalculator</code>","text":"<p>A class implementing an influence score based on reducing the dimension of the problem of computing IHVPs through the Arnoldi algorithm as per https://arxiv.org/pdf/2112.03052.pdf This allows this calculator to be used on models with a considerable amount of weights in a time-efficient manner. The influence score being calculated is theoretically the same as the rest of the calculators in the <code>influence</code> sub-package. </p>"},{"location":"api/influence/arnoldi/#__init__","title":"<code>__init__(self,  model:  deel.influenciae.common.model_wrappers.InfluenceModel,  train_dataset:  tf.Dataset,  subspace_dim:  int,  force_hermitian:  bool,  k_largest_eig_vals:  int,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 dtype:  = tf.float32) <p>Parameters</p> <ul> <li> <p>model            : deel.influenciae.common.model_wrappers.InfluenceModel </p> <ul> <li><p> The TF2.X model implementing the InfluenceModel interface.</p> </li> </ul> </li> <li> <p>train_dataset            : tf.Dataset </p> <ul> <li><p> A batched TF dataset with the points with which the model was trained.</p> </li> </ul> </li> <li> <p>subspace_dim            : int </p> <ul> <li><p> The dimension of the Krylov subspace for the Arnoldi algorithm.</p> </li> </ul> </li> <li> <p>force_hermitian            : bool </p> <ul> <li><p> A boolean indicating if we should force the projected matrix to be hermitian before the eigenvalue computation.</p> </li> </ul> </li> <li> <p>k_largest_eig_vals            : int </p> <ul> <li><p> An integer for the amount of top eigenvalues to keep for the influence estimations.</p> </li> </ul> </li> <li> <p>dtype            : \\Users\\lucas.hervier\\Anaconda3\\envs\\py39-tf212\\lib\\site-packages\\tensorflow\\_api\\v2\\dtypes\\init.py'&gt; = tf.float32 </p> <ul> <li><p> Numeric type for the Krylov basis (tf.float32 by default).</p> </li> </ul> </li> </ul>","text":""},{"location":"api/influence/arnoldi/#arnoldi","title":"<code>arnoldi(self,  dim:  int) -&gt; Tuple[tf.Tensor, tf.Tensor]</code> <p>Builds the projection of the inverse of the hessian on the Krylov subspaces. </p> <p>Parameters</p> <ul> <li> <p>dim            : int </p> <ul> <li><p> The dimension of the basis</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>eig_vals            : Tuple[tf.Tensor, tf.Tensor] </p> <ul> <li><p> The eigen values of the projection</p> </li> </ul> </li> <li> <p>G            : Tuple[tf.Tensor, tf.Tensor] </p> <ul> <li><p> The projection matrix</p> </li> </ul> </li> </ul> <p></p>","text":""},{"location":"api/influence/arnoldi/#compute_influence_values","title":"<code>compute_influence_values(self,  train_set:  tf.Dataset,  device:  Optional[str] = None) -&gt; tf.Dataset</code> <p>Compute the influence score for each sample of the provided (full or partial) model's training dataset. </p> <p>Parameters</p> <ul> <li> <p>train_set            : tf.Dataset </p> <ul> <li><p> A TF dataset with the (full or partial) model's training dataset.</p> </li> </ul> </li> <li> <p>device            : Optional[str] = None </p> <ul> <li><p> Device where the computation will be executed</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>train_set            : tf.Dataset </p> <ul> <li><p> A dataset containing the tuple: (batch of training samples, influence score)</p> </li> </ul> </li> </ul> <p></p>","text":""},{"location":"api/influence/arnoldi/#compute_influence_vector","title":"<code>compute_influence_vector(self,  train_set:  tf.Dataset,  save_influence_vector_ds_path:  Optional[str] = None,  device:  Optional[str] = None) -&gt; tf.Dataset</code> <p>Compute the influence vector for each sample of the provided (full or partial) model's training dataset. </p> <p>Parameters</p> <ul> <li> <p>train_set            : tf.Dataset </p> <ul> <li><p> A TF dataset with the (full or partial) model's training dataset.</p> </li> </ul> </li> <li> <p>save_influence_vector_ds_path            : Optional[str] = None </p> <ul> <li><p> The path to save or load the influence vector of the training dataset. If specified, load the dataset if it has already been computed, otherwise, compute the influence vector and then save it in the specified path.</p> </li> </ul> </li> <li> <p>device            : Optional[str] = None </p> <ul> <li><p> Device where the computation will be executed</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>inf_vect_ds            : tf.Dataset </p> <ul> <li><p> A dataset containing the tuple: (batch of training samples, influence vector)</p> </li> </ul> </li> </ul> <p></p>","text":""},{"location":"api/influence/arnoldi/#compute_top_k_from_training_dataset","title":"<code>compute_top_k_from_training_dataset(self,  train_set:  tf.Dataset,  k:  int,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 order: deel.influenciae.utils.sorted_dict.ORDER = ) -&gt; Tuple[tf.Tensor, tf.Tensor] <p>Compute the k most influential data-points of the model's training dataset by computing Cook's distance for each point individually. </p> <p>Parameters</p> <ul> <li> <p>train_set            : tf.Dataset </p> <ul> <li><p> A TF dataset containing the points on which the model was trained.</p> </li> </ul> </li> <li> <p>k            : int </p> <ul> <li><p> An integer with the number of most important samples we wish to keep</p> </li> </ul> </li> <li> <p>order            : 2&gt; </p> <ul> <li><p> Either ORDER.DESCENDING or ORDER.ASCENDING depending on if we wish to find the top-k or bottom-k samples, respectively.</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>training_samples, influences_values            : Tuple[tf.Tensor, tf.Tensor] </p> <ul> <li><p> A tuple of tensor.</p><p> - training_samples: A tensor containing the k most influential samples of the training dataset for the model provided.</p><p> - influences_values: The influence score corresponding to these k most influential samples.</p> </li> </ul> </li> </ul> <p></p>","text":""},{"location":"api/influence/arnoldi/#estimate_influence_values_in_batches","title":"<code>estimate_influence_values_in_batches(self,  dataset_to_evaluate:  tf.Dataset,  train_set:  tf.Dataset,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 influence_vector_in_cache: deel.influenciae.common.base_influence.CACHE = ,  load_influence_vector_path:  Optional[str] = None,  save_influence_vector_path:  Optional[str] = None,  save_influence_value_path:  Optional[str] = None,  device:  Optional[str] = None) -&gt; tf.Dataset <p>Estimates the influence that each point in the provided training dataset has on each of the test points. This can provide some insights as to what makes the model predict a certain way for the given test points, and thus presents data-centric explanations. </p> <p>Parameters</p> <ul> <li> <p>dataset_to_evaluate            : tf.Dataset </p> <ul> <li><p> A TF dataset containing the test samples for which to compute the effect of removing each of the provided training points (individually).</p> </li> </ul> </li> <li> <p>train_set            : tf.Dataset </p> <ul> <li><p> A TF dataset containing the model's training dataset (partial or full).</p> </li> </ul> </li> <li> <p>influence_vector_in_cache            : 0&gt; </p> <ul> <li><p> An enum indicating if intermediary values are to be cached (either in memory or on the disk) or not.</p><p> Options include CACHE.MEMORY (0) for caching in memory, CACHE.DISK (1) for the disk and CACHE.NO_CACHE (2) for no optimization.</p> </li> </ul> </li> <li> <p>load_influence_vector_path            : Optional[str] = None </p> <ul> <li><p> The path to load the influence vectors (if they have already been calculated).</p> </li> </ul> </li> <li> <p>save_influence_vector_path            : Optional[str] = None </p> <ul> <li><p> The path to save the computed influence vector.</p> </li> </ul> </li> <li> <p>save_influence_value_path            : Optional[str] = None </p> <ul> <li><p> The path to save the computed influence values.</p> </li> </ul> </li> <li> <p>device            : Optional[str] = None </p> <ul> <li><p> Device where the computation will be executed</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>influence_value_dataset            : tf.Dataset </p> <ul> <li><p> A dataset containing the tuple: (samples_to_evaluate, dataset).</p><p>  - samples_to_evaluate: The batch of sample to evaluate.</p><p> - dataset: Dataset containing tuples of batch of the training dataset and their influence score.</p> </li> </ul> </li> </ul> <p></p>","text":""},{"location":"api/influence/arnoldi/#top_k","title":"<code>top_k(self,  dataset_to_evaluate:  tf.Dataset,  train_set:  tf.Dataset,  k:  int = 5,  nearest_neighbors:  deel.influenciae.utils.nearest_neighbors.BaseNearestNeighbors = ,\u00a0\u00a0\u00a0\u00a0\u00a0 influence_vector_in_cache: deel.influenciae.common.base_influence.CACHE = ,  load_influence_vector_ds_path:  Optional[str] = None,  save_influence_vector_ds_path:  Optional[str] = None,  save_top_k_ds_path:  Optional[str] = None,\u00a0\u00a0\u00a0\u00a0\u00a0 order: deel.influenciae.utils.sorted_dict.ORDER = ,  d_type:  tensorflow.python.framework.dtypes.DType = tf.float32,  device:  Optional[str] = None) -&gt; tf.Dataset <p>Find the top-k closest elements for each element of dataset to evaluate in the training dataset The method will return a dataset containing a tuple of:     (Top-k influence values for each sample to evaluate, Top-k training sample for each sample to evaluate) </p> <p>Parameters</p> <ul> <li> <p>dataset_to_evaluate            : tf.Dataset </p> <ul> <li><p> The dataset which contains the samples which will be compare to the training dataset</p> </li> </ul> </li> <li> <p>train_set            : tf.Dataset </p> <ul> <li><p> The dataset used to train the model.</p> </li> </ul> </li> <li> <p>k            : int = 5 </p> <ul> <li><p> the number of most influence samples to retain in training dataset</p> </li> </ul> </li> <li> <p>nearest_neighbors            : deel.influenciae.utils.nearest_neighbors.BaseNearestNeighbors =   <ul> <li><p> The nearest neighbor method. The default method is a linear search</p> </li> </ul>  <li> <p>influence_vector_in_cache            : 0&gt; </p> <ul> <li><p> An enum indicating if intermediary values are to be cached (either in memory or on the disk) or not.</p><p> Options include CACHE.MEMORY (0) for caching in memory, CACHE.DISK (1) for the disk and CACHE.NO_CACHE (2) for no optimization.</p> </li> </ul> </li> <li> <p>load_influence_vector_ds_path            : Optional[str] = None </p> <ul> <li><p> The path to load the influence vectors (if they have already been calculated).</p> </li> </ul> </li> <li> <p>save_influence_vector_ds_path            : Optional[str] = None </p> <ul> <li><p> The path to save the computed influence vector.</p> </li> </ul> </li> <li> <p>save_top_k_ds_path            : Optional[str] = None </p> <ul> <li><p> The path to save the result of the computation of the top-k elements</p> </li> </ul> </li> <li> <p>order            : 2&gt; </p> <ul> <li><p> Either ORDER.DESCENDING or ORDER.ASCENDING depending on if we wish to find the top-k or bottom-k samples, respectively.</p> </li> </ul> </li> <li> <p>d_type            : tensorflow.python.framework.dtypes.DType = tf.float32 </p> <ul> <li><p> The data-type of the tensors.</p> </li> </ul> </li> <li> <p>device            : Optional[str] = None </p> <ul> <li><p> Device where the computation will be executed</p> </li> </ul> </li>  <p>Return</p> <ul> <li> <p>top_k_dataset            : tf.Dataset </p> <ul> <li><p> A dataset containing the tuple (samples_to_evaluate, influence_values, training_samples).</p><p>  - samples_to_evaluate: Top-k samples to evaluate.</p><p> - influence_values: Top-k influence values for each sample to evaluate.</p><p> - training_samples: Top-k training sample for each sample to evaluate.</p> </li> </ul> </li> </ul> <p></p>","text":""},{"location":"api/influence/first_order_influence_calculator/","title":"First Order Influence Calculator","text":"<p>View source | \ud83d\udcf0 Original Paper | \ud83d\udcf0 Paper Groups | \ud83d\udcf0 Paper RelatIF |</p> <p>This method is an implementation of the famous technique introduced by Koh &amp; Liang in 2017.  In essence, by performing a first-order taylor approximation, it proposes that the influence  function of a neural network model can be computed as follows:</p> \\[ \\mathcal{I} (z) \\approx H_{\\hat{\\theta}}^{-1} \\, \\nabla_\\theta \\ell (\\hat{\\theta}, z), \\] <p>where \\(H_{\\hat{\\theta}}^{-1}\\) is the inverse of the mean hessian of the loss wrt the model's parameters over the whole dataset, \\(\\ell\\) is the loss function with which the model was trained and \\(z\\), a point we wish to leave out of the training dataset.</p> <p>In particular, this computation is carried out by the <code>InverseHessianVectorProduct</code> class, which allows to do it in different ways, with each implementation having its pros and cons.</p> <p>It can be used to compute the self-influence of individual and groups of points, and the influence of training points (or groups) on other test points (or groups).</p> <p>It also implements the RelatIF technique, which can be computed by setting the <code>normalize</code> attribute to <code>True</code>.</p>"},{"location":"api/influence/first_order_influence_calculator/#notebooks","title":"Notebooks","text":"<ul> <li>Getting started</li> <li>Using the first order influence calculator</li> </ul>"},{"location":"api/influence/first_order_influence_calculator/#FirstOrderInfluenceCalculator","title":"<code>FirstOrderInfluenceCalculator</code>","text":"<p>A class implementing the necessary methods to compute the different influence quantities using a first-order approximation. This makes it ideal for individual points and small groups of data, as it does so (relatively) efficiently. </p>"},{"location":"api/influence/first_order_influence_calculator/#__init__","title":"<code>__init__(self,  model:  deel.influenciae.common.model_wrappers.InfluenceModel,  dataset:  tf.Dataset,  ihvp_calculator:  Union[str, deel.influenciae.common.inverse_hessian_vector_product.InverseHessianVectorProduct, deel.influenciae.common.inverse_hessian_vector_product.IHVPCalculator] = 'exact',  n_samples_for_hessian:  Optional[int] = None,  shuffle_buffer_size:  Optional[int] = 10000,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 normalize=False)</code>","text":"<p>Parameters</p> <ul> <li> <p>model            : deel.influenciae.common.model_wrappers.InfluenceModel </p> <ul> <li><p> The TF2.X model implementing the InfluenceModel interface.</p> </li> </ul> </li> <li> <p>dataset            : tf.Dataset </p> <ul> <li><p> A batched TF dataset containing the training dataset over which we will estimate the inverse-hessian-vector product.</p> </li> </ul> </li> <li> <p>ihvp_calculator            : Union[str, deel.influenciae.common.inverse_hessian_vector_product.InverseHessianVectorProduct, deel.influenciae.common.inverse_hessian_vector_product.IHVPCalculator] = 'exact' </p> <ul> <li><p> Either a string containing the IHVP method ('exact' or 'cgd'), an IHVPCalculator object or an InverseHessianVectorProduct object.</p> </li> </ul> </li> <li> <p>n_samples_for_hessian            : Optional[int] = None </p> <ul> <li><p> An integer indicating the amount of samples to take from the provided train dataset.</p> </li> </ul> </li> <li> <p>shuffle_buffer_size            : Optional[int] = 10000 </p> <ul> <li><p> An integer indicating the buffer size of the train dataset's shuffle operation -- when choosing the amount of samples for the hessian.</p> </li> </ul> </li> <li> <p>normalize            : normalize=False </p> <ul> <li><p> Implement \"RelatIF: Identifying Explanatory Training Examples via Relative Influence\" https://arxiv.org/pdf/2003.11630.pdf if True, compute the relative influence by normalizing the influence function.</p> </li> </ul> </li> </ul>"},{"location":"api/influence/first_order_influence_calculator/#assert_compatible_datasets","title":"<code>assert_compatible_datasets(dataset_a:  tf.Dataset,  dataset_b:  tf.Dataset) -&gt; int</code>","text":"<p>Assert that the datasets are compatible: that they contain the same number of points. Else, throw an error. </p> <p>Parameters</p> <ul> <li> <p>dataset_a            : tf.Dataset </p> <ul> <li><p> First batched tensorflow dataset to check.</p> </li> </ul> </li> <li> <p>dataset_b            : tf.Dataset </p> <ul> <li><p> Second batched tensorflow dataset to check.</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>size            : int </p> <ul> <li><p> The size of the dataset.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"api/influence/first_order_influence_calculator/#compute_influence_values","title":"<code>compute_influence_values(self,  train_set:  tf.Dataset,  device:  Optional[str] = None) -&gt; tf.Dataset</code>","text":"<p>Compute the influence score for each sample of the provided (full or partial) model's training dataset. </p> <p>Parameters</p> <ul> <li> <p>train_set            : tf.Dataset </p> <ul> <li><p> A TF dataset with the (full or partial) model's training dataset.</p> </li> </ul> </li> <li> <p>device            : Optional[str] = None </p> <ul> <li><p> Device where the computation will be executed</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>train_set            : tf.Dataset </p> <ul> <li><p> A dataset containing the tuple: (batch of training samples, influence score)</p> </li> </ul> </li> </ul> <p></p>"},{"location":"api/influence/first_order_influence_calculator/#compute_influence_vector","title":"<code>compute_influence_vector(self,  train_set:  tf.Dataset,  save_influence_vector_ds_path:  Optional[str] = None,  device:  Optional[str] = None) -&gt; tf.Dataset</code>","text":"<p>Compute the influence vector for each sample of the provided (full or partial) model's training dataset. </p> <p>Parameters</p> <ul> <li> <p>train_set            : tf.Dataset </p> <ul> <li><p> A TF dataset with the (full or partial) model's training dataset.</p> </li> </ul> </li> <li> <p>save_influence_vector_ds_path            : Optional[str] = None </p> <ul> <li><p> The path to save or load the influence vector of the training dataset. If specified, load the dataset if it has already been computed, otherwise, compute the influence vector and then save it in the specified path.</p> </li> </ul> </li> <li> <p>device            : Optional[str] = None </p> <ul> <li><p> Device where the computation will be executed</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>inf_vect_ds            : tf.Dataset </p> <ul> <li><p> A dataset containing the tuple: (batch of training samples, influence vector)</p> </li> </ul> </li> </ul> <p></p>"},{"location":"api/influence/first_order_influence_calculator/#compute_influence_vector_group","title":"<code>compute_influence_vector_group(self,  group:  tf.Dataset) -&gt; tf.Tensor</code>","text":"<p>Computes the influence function vector -- an estimation of the weights difference when removing the points -- of the whole group of points. </p> <p>Parameters</p> <ul> <li> <p>group            : tf.Dataset </p> <ul> <li><p> A batched TF dataset containing the group of points of which we wish to compute the influence of removal.</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>influence_group            : tf.Tensor </p> <ul> <li><p> A tensor containing one vector for the whole group.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"api/influence/first_order_influence_calculator/#compute_top_k_from_training_dataset","title":"<code>compute_top_k_from_training_dataset(self,  train_set:  tf.Dataset,  k:  int,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 order: deel.influenciae.utils.sorted_dict.ORDER = ) -&gt; Tuple[tf.Tensor, tf.Tensor] <p>Compute the k most influential data-points of the model's training dataset by computing Cook's distance for each point individually. </p> <p>Parameters</p> <ul> <li> <p>train_set            : tf.Dataset </p> <ul> <li><p> A TF dataset containing the points on which the model was trained.</p> </li> </ul> </li> <li> <p>k            : int </p> <ul> <li><p> An integer with the number of most important samples we wish to keep</p> </li> </ul> </li> <li> <p>order            : 2&gt; </p> <ul> <li><p> Either ORDER.DESCENDING or ORDER.ASCENDING depending on if we wish to find the top-k or bottom-k samples, respectively.</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>training_samples, influences_values            : Tuple[tf.Tensor, tf.Tensor] </p> <ul> <li><p> A tuple of tensor.</p><p> - training_samples: A tensor containing the k most influential samples of the training dataset for the model provided.</p><p> - influences_values: The influence score corresponding to these k most influential samples.</p> </li> </ul> </li> </ul> <p></p>","text":""},{"location":"api/influence/first_order_influence_calculator/#estimate_influence_values_group","title":"<code>estimate_influence_values_group(self,  group_train:  tf.Dataset,  group_to_evaluate:  Optional[tf.Dataset] = None) -&gt; tf.Tensor</code> <p>Computes Cook's distance of the whole group of points provided, giving measure of the influence that the group carries on the model's weights. </p> <p>Parameters</p> <ul> <li> <p>group_train            : tf.Dataset </p> <ul> <li><p> A batched TF dataset containing the group of points we wish to remove.</p> </li> </ul> </li> <li> <p>group_to_evaluate            : Optional[tf.Dataset] = None </p> <ul> <li><p> A batched TF dataset containing the group of points with respect to whom we wish to measure the influence of removing the training points.</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>influence_values_group            : tf.Tensor </p> <ul> <li><p> A tensor containing one influence value for the whole group.</p> </li> </ul> </li> </ul> <p></p>","text":""},{"location":"api/influence/first_order_influence_calculator/#estimate_influence_values_in_batches","title":"<code>estimate_influence_values_in_batches(self,  dataset_to_evaluate:  tf.Dataset,  train_set:  tf.Dataset,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 influence_vector_in_cache: deel.influenciae.common.base_influence.CACHE = ,  load_influence_vector_path:  Optional[str] = None,  save_influence_vector_path:  Optional[str] = None,  save_influence_value_path:  Optional[str] = None,  device:  Optional[str] = None) -&gt; tf.Dataset <p>Estimates the influence that each point in the provided training dataset has on each of the test points. This can provide some insights as to what makes the model predict a certain way for the given test points, and thus presents data-centric explanations. </p> <p>Parameters</p> <ul> <li> <p>dataset_to_evaluate            : tf.Dataset </p> <ul> <li><p> A TF dataset containing the test samples for which to compute the effect of removing each of the provided training points (individually).</p> </li> </ul> </li> <li> <p>train_set            : tf.Dataset </p> <ul> <li><p> A TF dataset containing the model's training dataset (partial or full).</p> </li> </ul> </li> <li> <p>influence_vector_in_cache            : 0&gt; </p> <ul> <li><p> An enum indicating if intermediary values are to be cached (either in memory or on the disk) or not.</p><p> Options include CACHE.MEMORY (0) for caching in memory, CACHE.DISK (1) for the disk and CACHE.NO_CACHE (2) for no optimization.</p> </li> </ul> </li> <li> <p>load_influence_vector_path            : Optional[str] = None </p> <ul> <li><p> The path to load the influence vectors (if they have already been calculated).</p> </li> </ul> </li> <li> <p>save_influence_vector_path            : Optional[str] = None </p> <ul> <li><p> The path to save the computed influence vector.</p> </li> </ul> </li> <li> <p>save_influence_value_path            : Optional[str] = None </p> <ul> <li><p> The path to save the computed influence values.</p> </li> </ul> </li> <li> <p>device            : Optional[str] = None </p> <ul> <li><p> Device where the computation will be executed</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>influence_value_dataset            : tf.Dataset </p> <ul> <li><p> A dataset containing the tuple: (samples_to_evaluate, dataset).</p><p>  - samples_to_evaluate: The batch of sample to evaluate.</p><p> - dataset: Dataset containing tuples of batch of the training dataset and their influence score.</p> </li> </ul> </li> </ul> <p></p>","text":""},{"location":"api/influence/first_order_influence_calculator/#top_k","title":"<code>top_k(self,  dataset_to_evaluate:  tf.Dataset,  train_set:  tf.Dataset,  k:  int = 5,  nearest_neighbors:  deel.influenciae.utils.nearest_neighbors.BaseNearestNeighbors = ,\u00a0\u00a0\u00a0\u00a0\u00a0 influence_vector_in_cache: deel.influenciae.common.base_influence.CACHE = ,  load_influence_vector_ds_path:  Optional[str] = None,  save_influence_vector_ds_path:  Optional[str] = None,  save_top_k_ds_path:  Optional[str] = None,\u00a0\u00a0\u00a0\u00a0\u00a0 order: deel.influenciae.utils.sorted_dict.ORDER = ,  d_type:  tensorflow.python.framework.dtypes.DType = tf.float32,  device:  Optional[str] = None) -&gt; tf.Dataset <p>Find the top-k closest elements for each element of dataset to evaluate in the training dataset The method will return a dataset containing a tuple of:     (Top-k influence values for each sample to evaluate, Top-k training sample for each sample to evaluate) </p> <p>Parameters</p> <ul> <li> <p>dataset_to_evaluate            : tf.Dataset </p> <ul> <li><p> The dataset which contains the samples which will be compare to the training dataset</p> </li> </ul> </li> <li> <p>train_set            : tf.Dataset </p> <ul> <li><p> The dataset used to train the model.</p> </li> </ul> </li> <li> <p>k            : int = 5 </p> <ul> <li><p> the number of most influence samples to retain in training dataset</p> </li> </ul> </li> <li> <p>nearest_neighbors            : deel.influenciae.utils.nearest_neighbors.BaseNearestNeighbors =   <ul> <li><p> The nearest neighbor method. The default method is a linear search</p> </li> </ul>  <li> <p>influence_vector_in_cache            : 0&gt; </p> <ul> <li><p> An enum indicating if intermediary values are to be cached (either in memory or on the disk) or not.</p><p> Options include CACHE.MEMORY (0) for caching in memory, CACHE.DISK (1) for the disk and CACHE.NO_CACHE (2) for no optimization.</p> </li> </ul> </li> <li> <p>load_influence_vector_ds_path            : Optional[str] = None </p> <ul> <li><p> The path to load the influence vectors (if they have already been calculated).</p> </li> </ul> </li> <li> <p>save_influence_vector_ds_path            : Optional[str] = None </p> <ul> <li><p> The path to save the computed influence vector.</p> </li> </ul> </li> <li> <p>save_top_k_ds_path            : Optional[str] = None </p> <ul> <li><p> The path to save the result of the computation of the top-k elements</p> </li> </ul> </li> <li> <p>order            : 2&gt; </p> <ul> <li><p> Either ORDER.DESCENDING or ORDER.ASCENDING depending on if we wish to find the top-k or bottom-k samples, respectively.</p> </li> </ul> </li> <li> <p>d_type            : tensorflow.python.framework.dtypes.DType = tf.float32 </p> <ul> <li><p> The data-type of the tensors.</p> </li> </ul> </li> <li> <p>device            : Optional[str] = None </p> <ul> <li><p> Device where the computation will be executed</p> </li> </ul> </li>  <p>Return</p> <ul> <li> <p>top_k_dataset            : tf.Dataset </p> <ul> <li><p> A dataset containing the tuple (samples_to_evaluate, influence_values, training_samples).</p><p>  - samples_to_evaluate: Top-k samples to evaluate.</p><p> - influence_values: Top-k influence values for each sample to evaluate.</p><p> - training_samples: Top-k training sample for each sample to evaluate.</p> </li> </ul> </li> </ul> <p></p>","text":""},{"location":"api/influence/second_order_influence_calculator/","title":"Second Order Influence Calculator","text":"<p>View source | \ud83d\udcf0 Original Paper</p> <p>When working with groups of data, it can prove useful to take into account the pairwise interactions in terms of influence when leaving out large groups of data-points. Basu et al. have thus introduced a second-order formulation that takes these interactions into account:</p> \\[ \\mathcal{I}^{(2)} (\\mathcal{U}, z_t) = \\nabla_\\theta \\ell (\\hat{\\theta}, z_t) \\left(\\mathcal{I}^{(1)} (\\mathcal{U}) + \\mathcal{I}' (\\mathcal{U})\\right) \\] \\[ \\mathcal{I}^{(1)} (\\mathcal{U}) = \\frac{1 - 2 p}{(1 - p)^2} \\frac{1}{|\\mathcal{S}|} H_{\\hat{\\theta}}^{-1} \\sum_{z \\in \\mathcal{U}} \\nabla_\\theta \\ell (\\hat{\\theta}, z) \\] \\[ \\mathcal{I}' (\\mathcal{U}) = \\frac{1}{(1 - p)^2} \\frac{1}{|S|^2} \\sum_{z \\in \\mathcal{U}} H_{\\hat{\\theta}}^{-1} \\nabla_\\theta^2 (\\hat{\\theta}, z) H_{\\hat{\\theta}}^{-1} \\sum_{z' \\in \\mathcal{U}} \\nabla_\\theta \\ell (\\hat{\\theta}, z')  \\] <p>As with the rest of the methods based on calculating inverse-hessian-vector products, an important part of the computations are carried out by objects from the class <code>InverseHessianVectorProduct</code>.</p>"},{"location":"api/influence/second_order_influence_calculator/#notebooks","title":"Notebooks","text":"<ul> <li>Using the second order influence calculator</li> </ul>"},{"location":"api/influence/second_order_influence_calculator/#SecondOrderInfluenceCalculator","title":"<code>SecondOrderInfluenceCalculator</code>","text":"<p>A class implementing the necessary methods to compute the different influence quantities (only for groups) using a second-order approximation, thus allowing us to take into account the pairwise interactions between points inside the group. For small groups of points, consider using the first order alternative if the computational cost is too high. </p>"},{"location":"api/influence/second_order_influence_calculator/#__init__","title":"<code>__init__(self,  model:  deel.influenciae.common.model_wrappers.InfluenceModel,  dataset:  tf.Dataset,  ihvp_calculator:  Union[str, deel.influenciae.common.inverse_hessian_vector_product.InverseHessianVectorProduct, deel.influenciae.common.inverse_hessian_vector_product.IHVPCalculator] = 'exact',  n_samples_for_hessian:  Optional[int] = None,  shuffle_buffer_size:  Optional[int] = 10000)</code>","text":"<p>Parameters</p> <ul> <li> <p>model            : deel.influenciae.common.model_wrappers.InfluenceModel </p> <ul> <li><p> The TF2.X model implementing the InfluenceModel interface.</p> </li> </ul> </li> <li> <p>dataset            : tf.Dataset </p> <ul> <li><p> A batched TF dataset containing the training dataset over which we will estimate the inverse-hessian-vector product.</p> </li> </ul> </li> <li> <p>ihvp_calculator            : Union[str, deel.influenciae.common.inverse_hessian_vector_product.InverseHessianVectorProduct, deel.influenciae.common.inverse_hessian_vector_product.IHVPCalculator] = 'exact' </p> <ul> <li><p> Either a string containing the IHVP method ('exact' or 'cgd'), an IHVPCalculator object or an InverseHessianVectorProduct object.</p> </li> </ul> </li> <li> <p>n_samples_for_hessian            : Optional[int] = None </p> <ul> <li><p> An integer indicating the amount of samples to take from the provided train dataset.</p> </li> </ul> </li> <li> <p>shuffle_buffer_size            : Optional[int] = 10000 </p> <ul> <li><p> An integer indicating the buffer size of the train dataset's shuffle operation -- when choosing the amount of samples for the hessian.</p> </li> </ul> </li> </ul>"},{"location":"api/influence/second_order_influence_calculator/#assert_compatible_datasets","title":"<code>assert_compatible_datasets(dataset_a:  tf.Dataset,  dataset_b:  tf.Dataset) -&gt; int</code>","text":"<p>Assert that the datasets are compatible: that they contain the same number of points. Else, throw an error. </p> <p>Parameters</p> <ul> <li> <p>dataset_a            : tf.Dataset </p> <ul> <li><p> First batched tensorflow dataset to check.</p> </li> </ul> </li> <li> <p>dataset_b            : tf.Dataset </p> <ul> <li><p> Second batched tensorflow dataset to check.</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>size            : int </p> <ul> <li><p> The size of the dataset.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"api/influence/second_order_influence_calculator/#compute_influence_vector_group","title":"<code>compute_influence_vector_group(self,  group:  tf.Dataset) -&gt; tf.Tensor</code>","text":"<p>Computes the influence function vector -- an estimation of the weights difference when removing the points -- of the whole group of points. </p> <p>Parameters</p> <ul> <li> <p>group            : tf.Dataset </p> <ul> <li><p> A batched TF dataset containing the group of points of which we wish to compute the influence of removal.</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>influence_group            : tf.Tensor </p> <ul> <li><p> A tensor containing one vector for the whole group.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"api/influence/second_order_influence_calculator/#estimate_influence_values_group","title":"<code>estimate_influence_values_group(self,  group_train:  tf.Dataset,  group_to_evaluate:  Optional[tf.Dataset] = None) -&gt; tf.Tensor</code>","text":"<p>Computes Cook's distance of the whole group of points provided, giving measure of the influence that the group carries on the model's weights. </p> <p>Parameters</p> <ul> <li> <p>group_train            : tf.Dataset </p> <ul> <li><p> A batched TF dataset containing the group of points we wish to remove.</p> </li> </ul> </li> <li> <p>group_to_evaluate            : Optional[tf.Dataset] = None </p> <ul> <li><p> A batched TF dataset containing the group of points with respect to whom we wish to measure the influence of removing the training points.</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>influence_values_group            : tf.Tensor </p> <ul> <li><p> A tensor containing one influence value for the whole group.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"api/representer_point_selection/rps_l2/","title":"Representer Point Selection - L2","text":"<p>View source | \ud83d\udcf0 Original Paper</p> <p>Using a completely different notion of influence than the techniques in modules <code>deel.influenciae.influence</code> and <code>deel.influenciae.trac_in</code>, this is the first method to use the representer point theorem for kernels to attribute an influence to data-points in the training dataset. In particular, it posits that the classification function can be approximated by:</p> \\[ \\Phi (x_t, \\theta) = \\sum_i^n k (x_t, x_i, \\alpha_i) \\] \\[ \\alpha_i = - \\frac{1}{2 \\lambda n} \\frac{\\partial L (x_i, y_i, \\theta)}{\\partial \\Phi (x_i, \\theta)} \\] <p>where \\(\\Phi\\) is the function that transforms the points \\(x_t\\) in the embedding of the network's last layer to the logits for the classification. However, this function must be learned with a strong \\(\\ell^2\\) regularization, and thus requires creating a surrogate model.</p> <p>In particular, it will be the \\(\\alpha_i\\) for the predicted label \\(j\\) the equivalent of the influence of the data-point.</p> <p>This method does not require us to compute inverse-hessian-vector products, so it can be computed with a certain efficiency once the surrogate model has been learned.</p>"},{"location":"api/representer_point_selection/rps_l2/#notebooks","title":"Notebooks","text":"<ul> <li>Using Representer Point Selection - L2 (RPS_L2)</li> </ul>"},{"location":"api/representer_point_selection/rps_l2/#RepresenterPointL2","title":"<code>RepresenterPointL2</code>","text":"<p>A class implementing a method to compute the influence of training points through the representer point theorem for kernels. </p>"},{"location":"api/representer_point_selection/rps_l2/#__init__","title":"<code>__init__(self,  model:  keras.engine.training.Model,  train_set:  tf.Dataset,  loss_function:  Union[Callable[[tf.Tensor, tf.Tensor], tf.Tensor], keras.losses.Loss],  lambda_regularization:  float,  scaling_factor:  float = 0.1,  epochs:  int = 100,  layer_index:  int = -1)</code>","text":"<p>Parameters</p> <ul> <li> <p>model            : keras.engine.training.Model </p> <ul> <li><p> A TF2 model that has already been trained</p> </li> </ul> </li> <li> <p>train_set            : tf.Dataset </p> <ul> <li><p> A batched TF dataset with the points with which the model was trained</p> </li> </ul> </li> <li> <p>loss_function            : Union[Callable[[tf.Tensor, tf.Tensor], tf.Tensor], keras.losses.Loss] </p> <ul> <li><p> The loss function with which the model was trained. This loss function MUST NOT be reduced.</p> </li> </ul> </li> <li> <p>lambda_regularization            : float </p> <ul> <li><p> The coefficient for the regularization of the surrogate last layer that needs to be trained for this method</p> </li> </ul> </li> <li> <p>scaling_factor            : float = 0.1 </p> <ul> <li><p> A float with the scaling factor for the SGD backtracking line-search optimizer for fitting the surrogate linear model</p> </li> </ul> </li> <li> <p>epochs            : int = 100 </p> <ul> <li><p> An integer for the amount of epochs to fit the linear model</p> </li> </ul> </li> <li> <p>layer_index            : int = -1 </p> <ul> <li><p> layer of the logits</p> </li> </ul> </li> </ul>"},{"location":"api/representer_point_selection/rps_l2/#compute_influence_values","title":"<code>compute_influence_values(self,  train_set:  tf.Dataset,  device:  Optional[str] = None) -&gt; tf.Dataset</code>","text":"<p>Compute the influence score for each sample of the provided (full or partial) model's training dataset. </p> <p>Parameters</p> <ul> <li> <p>train_set            : tf.Dataset </p> <ul> <li><p> A TF dataset with the (full or partial) model's training dataset.</p> </li> </ul> </li> <li> <p>device            : Optional[str] = None </p> <ul> <li><p> Device where the computation will be executed</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>train_set            : tf.Dataset </p> <ul> <li><p> A dataset containing the tuple: (batch of training samples, influence score)</p> </li> </ul> </li> </ul> <p></p>"},{"location":"api/representer_point_selection/rps_l2/#compute_influence_vector","title":"<code>compute_influence_vector(self,  train_set:  tf.Dataset,  save_influence_vector_ds_path:  Optional[str] = None,  device:  Optional[str] = None) -&gt; tf.Dataset</code>","text":"<p>Compute the influence vector for each sample of the provided (full or partial) model's training dataset. </p> <p>Parameters</p> <ul> <li> <p>train_set            : tf.Dataset </p> <ul> <li><p> A TF dataset with the (full or partial) model's training dataset.</p> </li> </ul> </li> <li> <p>save_influence_vector_ds_path            : Optional[str] = None </p> <ul> <li><p> The path to save or load the influence vector of the training dataset. If specified, load the dataset if it has already been computed, otherwise, compute the influence vector and then save it in the specified path.</p> </li> </ul> </li> <li> <p>device            : Optional[str] = None </p> <ul> <li><p> Device where the computation will be executed</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>inf_vect_ds            : tf.Dataset </p> <ul> <li><p> A dataset containing the tuple: (batch of training samples, influence vector)</p> </li> </ul> </li> </ul> <p></p>"},{"location":"api/representer_point_selection/rps_l2/#compute_top_k_from_training_dataset","title":"<code>compute_top_k_from_training_dataset(self,  train_set:  tf.Dataset,  k:  int,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 order: deel.influenciae.utils.sorted_dict.ORDER = ) -&gt; Tuple[tf.Tensor, tf.Tensor] <p>Compute the k most influential data-points of the model's training dataset by computing Cook's distance for each point individually. </p> <p>Parameters</p> <ul> <li> <p>train_set            : tf.Dataset </p> <ul> <li><p> A TF dataset containing the points on which the model was trained.</p> </li> </ul> </li> <li> <p>k            : int </p> <ul> <li><p> An integer with the number of most important samples we wish to keep</p> </li> </ul> </li> <li> <p>order            : 2&gt; </p> <ul> <li><p> Either ORDER.DESCENDING or ORDER.ASCENDING depending on if we wish to find the top-k or bottom-k samples, respectively.</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>training_samples, influences_values            : Tuple[tf.Tensor, tf.Tensor] </p> <ul> <li><p> A tuple of tensor.</p><p> - training_samples: A tensor containing the k most influential samples of the training dataset for the model provided.</p><p> - influences_values: The influence score corresponding to these k most influential samples.</p> </li> </ul> </li> </ul> <p></p>","text":""},{"location":"api/representer_point_selection/rps_l2/#estimate_influence_values_in_batches","title":"<code>estimate_influence_values_in_batches(self,  dataset_to_evaluate:  tf.Dataset,  train_set:  tf.Dataset,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 influence_vector_in_cache: deel.influenciae.common.base_influence.CACHE = ,  load_influence_vector_path:  Optional[str] = None,  save_influence_vector_path:  Optional[str] = None,  save_influence_value_path:  Optional[str] = None,  device:  Optional[str] = None) -&gt; tf.Dataset <p>Estimates the influence that each point in the provided training dataset has on each of the test points. This can provide some insights as to what makes the model predict a certain way for the given test points, and thus presents data-centric explanations. </p> <p>Parameters</p> <ul> <li> <p>dataset_to_evaluate            : tf.Dataset </p> <ul> <li><p> A TF dataset containing the test samples for which to compute the effect of removing each of the provided training points (individually).</p> </li> </ul> </li> <li> <p>train_set            : tf.Dataset </p> <ul> <li><p> A TF dataset containing the model's training dataset (partial or full).</p> </li> </ul> </li> <li> <p>influence_vector_in_cache            : 0&gt; </p> <ul> <li><p> An enum indicating if intermediary values are to be cached (either in memory or on the disk) or not.</p><p> Options include CACHE.MEMORY (0) for caching in memory, CACHE.DISK (1) for the disk and CACHE.NO_CACHE (2) for no optimization.</p> </li> </ul> </li> <li> <p>load_influence_vector_path            : Optional[str] = None </p> <ul> <li><p> The path to load the influence vectors (if they have already been calculated).</p> </li> </ul> </li> <li> <p>save_influence_vector_path            : Optional[str] = None </p> <ul> <li><p> The path to save the computed influence vector.</p> </li> </ul> </li> <li> <p>save_influence_value_path            : Optional[str] = None </p> <ul> <li><p> The path to save the computed influence values.</p> </li> </ul> </li> <li> <p>device            : Optional[str] = None </p> <ul> <li><p> Device where the computation will be executed</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>influence_value_dataset            : tf.Dataset </p> <ul> <li><p> A dataset containing the tuple: (samples_to_evaluate, dataset).</p><p>  - samples_to_evaluate: The batch of sample to evaluate.</p><p> - dataset: Dataset containing tuples of batch of the training dataset and their influence score.</p> </li> </ul> </li> </ul> <p></p>","text":""},{"location":"api/representer_point_selection/rps_l2/#predict_with_kernel","title":"<code>predict_with_kernel(self,  samples_to_evaluate:  Tuple[tf.Tensor, ...]) -&gt; tf.Tensor</code> <p>Uses the learned kernel to approximate the model's predictions on a group of samples. </p> <p>Parameters</p> <ul> <li> <p>samples_to_evaluate            : Tuple[tf.Tensor, ...] </p> <ul> <li><p> A single batch of tensors with the samples for which we wish to approximate the model's predictions</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>predictions            : tf.Tensor </p> <ul> <li><p> A tensor with an approximation of the model's predictions</p> </li> </ul> </li> </ul> <p></p>","text":""},{"location":"api/representer_point_selection/rps_l2/#top_k","title":"<code>top_k(self,  dataset_to_evaluate:  tf.Dataset,  train_set:  tf.Dataset,  k:  int = 5,  nearest_neighbors:  deel.influenciae.utils.nearest_neighbors.BaseNearestNeighbors = ,\u00a0\u00a0\u00a0\u00a0\u00a0 influence_vector_in_cache: deel.influenciae.common.base_influence.CACHE = ,  load_influence_vector_ds_path:  Optional[str] = None,  save_influence_vector_ds_path:  Optional[str] = None,  save_top_k_ds_path:  Optional[str] = None,\u00a0\u00a0\u00a0\u00a0\u00a0 order: deel.influenciae.utils.sorted_dict.ORDER = ,  d_type:  tensorflow.python.framework.dtypes.DType = tf.float32,  device:  Optional[str] = None) -&gt; tf.Dataset <p>Find the top-k closest elements for each element of dataset to evaluate in the training dataset The method will return a dataset containing a tuple of:     (Top-k influence values for each sample to evaluate, Top-k training sample for each sample to evaluate) </p> <p>Parameters</p> <ul> <li> <p>dataset_to_evaluate            : tf.Dataset </p> <ul> <li><p> The dataset which contains the samples which will be compare to the training dataset</p> </li> </ul> </li> <li> <p>train_set            : tf.Dataset </p> <ul> <li><p> The dataset used to train the model.</p> </li> </ul> </li> <li> <p>k            : int = 5 </p> <ul> <li><p> the number of most influence samples to retain in training dataset</p> </li> </ul> </li> <li> <p>nearest_neighbors            : deel.influenciae.utils.nearest_neighbors.BaseNearestNeighbors =   <ul> <li><p> The nearest neighbor method. The default method is a linear search</p> </li> </ul>  <li> <p>influence_vector_in_cache            : 0&gt; </p> <ul> <li><p> An enum indicating if intermediary values are to be cached (either in memory or on the disk) or not.</p><p> Options include CACHE.MEMORY (0) for caching in memory, CACHE.DISK (1) for the disk and CACHE.NO_CACHE (2) for no optimization.</p> </li> </ul> </li> <li> <p>load_influence_vector_ds_path            : Optional[str] = None </p> <ul> <li><p> The path to load the influence vectors (if they have already been calculated).</p> </li> </ul> </li> <li> <p>save_influence_vector_ds_path            : Optional[str] = None </p> <ul> <li><p> The path to save the computed influence vector.</p> </li> </ul> </li> <li> <p>save_top_k_ds_path            : Optional[str] = None </p> <ul> <li><p> The path to save the result of the computation of the top-k elements</p> </li> </ul> </li> <li> <p>order            : 2&gt; </p> <ul> <li><p> Either ORDER.DESCENDING or ORDER.ASCENDING depending on if we wish to find the top-k or bottom-k samples, respectively.</p> </li> </ul> </li> <li> <p>d_type            : tensorflow.python.framework.dtypes.DType = tf.float32 </p> <ul> <li><p> The data-type of the tensors.</p> </li> </ul> </li> <li> <p>device            : Optional[str] = None </p> <ul> <li><p> Device where the computation will be executed</p> </li> </ul> </li>  <p>Return</p> <ul> <li> <p>top_k_dataset            : tf.Dataset </p> <ul> <li><p> A dataset containing the tuple (samples_to_evaluate, influence_values, training_samples).</p><p>  - samples_to_evaluate: Top-k samples to evaluate.</p><p> - influence_values: Top-k influence values for each sample to evaluate.</p><p> - training_samples: Top-k training sample for each sample to evaluate.</p> </li> </ul> </li> </ul> <p></p>","text":""},{"location":"api/representer_point_selection/rps_lje/","title":"Representer Point Selection - Local Jacobian Expansion","text":"<p>View source | \ud83d\udcf0 Original Paper</p> <p>Introduced as an improvement over Representer Point Selection - L2, this technique trades the surrogate model for a local taylor expansion on the jacobian matrix, effectively allowing for the decomposition of the model's last layer into a kernel as an approximation.  In short, it proposes the following formula for computing influence values:</p> \\[ \\Theta_L^\\dagger \\phi (x_t) = \\sum_i \\alpha_i \\phi (x_i)^T \\phi (x_y) \\] \\[ \\alpha_i = \\Theta_L \\frac{1}{\\phi (x_i) \\, n} - \\frac{1}{n} H_{\\Theta_L}^{-1} \\frac{\\partial L (x_i, y_i, \\Theta)}{\\partial \\Theta_L \\phi (x_i)} \\] <p>In particular, it will be the \\(\\alpha_i\\) for the predicted label \\(j\\) the equivalent of the influence of the data-point.</p> <p>As all the other methods based on computing inverse-hessian-vector products, it will be performing all these computations with the help of objects of the class <code>InverseHessianVectorProduct</code>, capable of doing so efficiently.</p>"},{"location":"api/representer_point_selection/rps_lje/#notebooks","title":"Notebooks","text":"<ul> <li>Using Representer Point Selection - Local Jacobian Expansion (RPS_LJE)</li> </ul>"},{"location":"api/representer_point_selection/rps_lje/#RepresenterPointLJE","title":"<code>RepresenterPointLJE</code>","text":"<p>Representer Point Selection via Local Jacobian Expansion for Post-hoc Classifier Explanation of Deep Neural Networks and Ensemble Models https://proceedings.neurips.cc/paper/2021/file/c460dc0f18fc309ac07306a4a55d2fd6-Paper.pdf </p>"},{"location":"api/representer_point_selection/rps_lje/#__init__","title":"<code>__init__(self,  influence_model:  deel.influenciae.common.model_wrappers.InfluenceModel,  dataset:  tf.Dataset,  ihvp_calculator_factory:  deel.influenciae.common.ihvp_factory.InverseHessianVectorProductFactory,  n_samples_for_hessian:  Optional[int] = None,  target_layer:  Union[int, str] = -1,  shuffle_buffer_size:  int = 10000,  epsilon:  float = 1e-05)</code>","text":"<p>Parameters</p> <ul> <li> <p>influence_model            : deel.influenciae.common.model_wrappers.InfluenceModel </p> <ul> <li><p> The TF2.X model implementing the InfluenceModel interface.</p> </li> </ul> </li> <li> <p>ihvp_calculator_factory            : deel.influenciae.common.ihvp_factory.InverseHessianVectorProductFactory </p> <ul> <li><p> An InverseHessianVectorProductFactory for creating new instances of the InverseHessianVectorProduct class.</p> </li> </ul> </li> <li> <p>n_samples_for_hessian            : Optional[int] = None </p> <ul> <li><p> An integer for the amount of samples from the training dataset that will be used for the computation of the hessian matrix.</p><p> If None, the whole dataset will be used.</p> </li> </ul> </li> <li> <p>target_layer            : Union[int, str] = -1 </p> <ul> <li><p> Either a string or an integer identifying the layer on which to compute the influence-related quantities.</p> </li> </ul> </li> <li> <p>shuffle_buffer_size            : int = 10000 </p> <ul> <li><p> An integer with the buffer size for the training set's shuffle operation.</p> </li> </ul> </li> <li> <p>epsilon            : float = 1e-05 </p> <ul> <li><p> An epsilon value to prevent division by zero.</p> </li> </ul> </li> </ul>"},{"location":"api/representer_point_selection/rps_lje/#compute_influence_values","title":"<code>compute_influence_values(self,  train_set:  tf.Dataset,  device:  Optional[str] = None) -&gt; tf.Dataset</code>","text":"<p>Compute the influence score for each sample of the provided (full or partial) model's training dataset. </p> <p>Parameters</p> <ul> <li> <p>train_set            : tf.Dataset </p> <ul> <li><p> A TF dataset with the (full or partial) model's training dataset.</p> </li> </ul> </li> <li> <p>device            : Optional[str] = None </p> <ul> <li><p> Device where the computation will be executed</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>train_set            : tf.Dataset </p> <ul> <li><p> A dataset containing the tuple: (batch of training samples, influence score)</p> </li> </ul> </li> </ul> <p></p>"},{"location":"api/representer_point_selection/rps_lje/#compute_influence_vector","title":"<code>compute_influence_vector(self,  train_set:  tf.Dataset,  save_influence_vector_ds_path:  Optional[str] = None,  device:  Optional[str] = None) -&gt; tf.Dataset</code>","text":"<p>Compute the influence vector for each sample of the provided (full or partial) model's training dataset. </p> <p>Parameters</p> <ul> <li> <p>train_set            : tf.Dataset </p> <ul> <li><p> A TF dataset with the (full or partial) model's training dataset.</p> </li> </ul> </li> <li> <p>save_influence_vector_ds_path            : Optional[str] = None </p> <ul> <li><p> The path to save or load the influence vector of the training dataset. If specified, load the dataset if it has already been computed, otherwise, compute the influence vector and then save it in the specified path.</p> </li> </ul> </li> <li> <p>device            : Optional[str] = None </p> <ul> <li><p> Device where the computation will be executed</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>inf_vect_ds            : tf.Dataset </p> <ul> <li><p> A dataset containing the tuple: (batch of training samples, influence vector)</p> </li> </ul> </li> </ul> <p></p>"},{"location":"api/representer_point_selection/rps_lje/#compute_top_k_from_training_dataset","title":"<code>compute_top_k_from_training_dataset(self,  train_set:  tf.Dataset,  k:  int,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 order: deel.influenciae.utils.sorted_dict.ORDER = ) -&gt; Tuple[tf.Tensor, tf.Tensor] <p>Compute the k most influential data-points of the model's training dataset by computing Cook's distance for each point individually. </p> <p>Parameters</p> <ul> <li> <p>train_set            : tf.Dataset </p> <ul> <li><p> A TF dataset containing the points on which the model was trained.</p> </li> </ul> </li> <li> <p>k            : int </p> <ul> <li><p> An integer with the number of most important samples we wish to keep</p> </li> </ul> </li> <li> <p>order            : 2&gt; </p> <ul> <li><p> Either ORDER.DESCENDING or ORDER.ASCENDING depending on if we wish to find the top-k or bottom-k samples, respectively.</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>training_samples, influences_values            : Tuple[tf.Tensor, tf.Tensor] </p> <ul> <li><p> A tuple of tensor.</p><p> - training_samples: A tensor containing the k most influential samples of the training dataset for the model provided.</p><p> - influences_values: The influence score corresponding to these k most influential samples.</p> </li> </ul> </li> </ul> <p></p>","text":""},{"location":"api/representer_point_selection/rps_lje/#estimate_influence_values_in_batches","title":"<code>estimate_influence_values_in_batches(self,  dataset_to_evaluate:  tf.Dataset,  train_set:  tf.Dataset,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 influence_vector_in_cache: deel.influenciae.common.base_influence.CACHE = ,  load_influence_vector_path:  Optional[str] = None,  save_influence_vector_path:  Optional[str] = None,  save_influence_value_path:  Optional[str] = None,  device:  Optional[str] = None) -&gt; tf.Dataset <p>Estimates the influence that each point in the provided training dataset has on each of the test points. This can provide some insights as to what makes the model predict a certain way for the given test points, and thus presents data-centric explanations. </p> <p>Parameters</p> <ul> <li> <p>dataset_to_evaluate            : tf.Dataset </p> <ul> <li><p> A TF dataset containing the test samples for which to compute the effect of removing each of the provided training points (individually).</p> </li> </ul> </li> <li> <p>train_set            : tf.Dataset </p> <ul> <li><p> A TF dataset containing the model's training dataset (partial or full).</p> </li> </ul> </li> <li> <p>influence_vector_in_cache            : 0&gt; </p> <ul> <li><p> An enum indicating if intermediary values are to be cached (either in memory or on the disk) or not.</p><p> Options include CACHE.MEMORY (0) for caching in memory, CACHE.DISK (1) for the disk and CACHE.NO_CACHE (2) for no optimization.</p> </li> </ul> </li> <li> <p>load_influence_vector_path            : Optional[str] = None </p> <ul> <li><p> The path to load the influence vectors (if they have already been calculated).</p> </li> </ul> </li> <li> <p>save_influence_vector_path            : Optional[str] = None </p> <ul> <li><p> The path to save the computed influence vector.</p> </li> </ul> </li> <li> <p>save_influence_value_path            : Optional[str] = None </p> <ul> <li><p> The path to save the computed influence values.</p> </li> </ul> </li> <li> <p>device            : Optional[str] = None </p> <ul> <li><p> Device where the computation will be executed</p> </li> </ul> </li> </ul> <p>Return</p> <ul> <li> <p>influence_value_dataset            : tf.Dataset </p> <ul> <li><p> A dataset containing the tuple: (samples_to_evaluate, dataset).</p><p>  - samples_to_evaluate: The batch of sample to evaluate.</p><p> - dataset: Dataset containing tuples of batch of the training dataset and their influence score.</p> </li> </ul> </li> </ul> <p></p>","text":""},{"location":"api/representer_point_selection/rps_lje/#top_k","title":"<code>top_k(self,  dataset_to_evaluate:  tf.Dataset,  train_set:  tf.Dataset,  k:  int = 5,  nearest_neighbors:  deel.influenciae.utils.nearest_neighbors.BaseNearestNeighbors = ,\u00a0\u00a0\u00a0\u00a0\u00a0 influence_vector_in_cache: deel.influenciae.common.base_influence.CACHE = ,  load_influence_vector_ds_path:  Optional[str] = None,  save_influence_vector_ds_path:  Optional[str] = None,  save_top_k_ds_path:  Optional[str] = None,\u00a0\u00a0\u00a0\u00a0\u00a0 order: deel.influenciae.utils.sorted_dict.ORDER = ,  d_type:  tensorflow.python.framework.dtypes.DType = tf.float32,  device:  Optional[str] = None) -&gt; tf.Dataset <p>Find the top-k closest elements for each element of dataset to evaluate in the training dataset The method will return a dataset containing a tuple of:     (Top-k influence values for each sample to evaluate, Top-k training sample for each sample to evaluate) </p> <p>Parameters</p> <ul> <li> <p>dataset_to_evaluate            : tf.Dataset </p> <ul> <li><p> The dataset which contains the samples which will be compare to the training dataset</p> </li> </ul> </li> <li> <p>train_set            : tf.Dataset </p> <ul> <li><p> The dataset used to train the model.</p> </li> </ul> </li> <li> <p>k            : int = 5 </p> <ul> <li><p> the number of most influence samples to retain in training dataset</p> </li> </ul> </li> <li> <p>nearest_neighbors            : deel.influenciae.utils.nearest_neighbors.BaseNearestNeighbors =   <ul> <li><p> The nearest neighbor method. The default method is a linear search</p> </li> </ul>  <li> <p>influence_vector_in_cache            : 0&gt; </p> <ul> <li><p> An enum indicating if intermediary values are to be cached (either in memory or on the disk) or not.</p><p> Options include CACHE.MEMORY (0) for caching in memory, CACHE.DISK (1) for the disk and CACHE.NO_CACHE (2) for no optimization.</p> </li> </ul> </li> <li> <p>load_influence_vector_ds_path            : Optional[str] = None </p> <ul> <li><p> The path to load the influence vectors (if they have already been calculated).</p> </li> </ul> </li> <li> <p>save_influence_vector_ds_path            : Optional[str] = None </p> <ul> <li><p> The path to save the computed influence vector.</p> </li> </ul> </li> <li> <p>save_top_k_ds_path            : Optional[str] = None </p> <ul> <li><p> The path to save the result of the computation of the top-k elements</p> </li> </ul> </li> <li> <p>order            : 2&gt; </p> <ul> <li><p> Either ORDER.DESCENDING or ORDER.ASCENDING depending on if we wish to find the top-k or bottom-k samples, respectively.</p> </li> </ul> </li> <li> <p>d_type            : tensorflow.python.framework.dtypes.DType = tf.float32 </p> <ul> <li><p> The data-type of the tensors.</p> </li> </ul> </li> <li> <p>device            : Optional[str] = None </p> <ul> <li><p> Device where the computation will be executed</p> </li> </ul> </li>  <p>Return</p> <ul> <li> <p>top_k_dataset            : tf.Dataset </p> <ul> <li><p> A dataset containing the tuple (samples_to_evaluate, influence_values, training_samples).</p><p>  - samples_to_evaluate: Top-k samples to evaluate.</p><p> - influence_values: Top-k influence values for each sample to evaluate.</p><p> - training_samples: Top-k training sample for each sample to evaluate.</p> </li> </ul> </li> </ul> <p></p>","text":""}]}